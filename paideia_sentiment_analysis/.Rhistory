my_data <- data.frame(my_data)
my_data$Style[my_data$Style == "positivr"] = "positive"
my_data
my_data <- data.frame(my_data)
sw <- stopwords("it")
testi <- my_data$Text
#tokenizzazione
tokens_full <- tokenize_words(testi)
testi <- my_data$Text
#tokenizzazione
tokens_full <- tokenize_words(testi)
tokens_full <- unlist(tokens_full)
tokens_full <- tokens_full[!tokens_full %in% sw]
#tokenizzazione
tokens_full <- tokenize_words(testi)
tokens_full <- unlist(tokens_full)
tokens_full <- tokens_full[!tokens_full %in% sw]
class(tokens_full)
# ottenere la frequenza dei token
freq_parole <- as.data.frame(table(tokens_full))
freq_parole_ordinate <- freq_parole[order(-freq_parole$Freq),]
head(freq_parole_ordinate,10)
#ricarichiamo il nostro file di testo
my_data <- read_excel("../materiali/dataset.xlsx")
my_data <- data.frame(my_data)
testi <- my_data$Text
#tokenizzazione
tokens_full <- tokenize_words(testi)
tokens_full <- unlist(tokens_full)
class(tokens_full)
# ottenere la frequenza dei token
freq_parole <- as.data.frame(table(tokens_full))
freq_parole_ordinate <- freq_parole[order(-freq_parole$Freq),]
head(freq_parole_ordinate,10)
# ottenere la frequenza dei token
freq_parole <- as.data.frame(table(tokens_full))
freq_parole_ordinate <- freq_parole[order(-freq_parole$Freq),]
# esaminare una parola di interesse
parola_di_interesse <- "ottimo"
tokens==parola_di_interesse
num_occorrenze <- sum(tokens==parola_di_interesse)
cat("La parola", parola_di_interesse, " compare ", num_occorrenze, " volte nel testo")
num_occorrenze <- sum(tokens_full==parola_di_interesse)
num_occorrenze <- freq_parole[freq_parole$tokens_full == parola_di_interesse]
num_occorrenze <- freq_parole[freq_parole$tokens_full == parola_di_interesse,]
num_occorrenze
num_occorrenze <- sum(tokens_full==parola_di_interesse)
cat("La parola", parola_di_interesse, " compare ", num_occorrenze, " volte nel testo")
# esaminare una parola di interesse
parola_di_interesse <- "salvini"
tokens==parola_di_interesse
num_occorrenze <- freq_parole[freq_parole$tokens_full == parola_di_interesse,]
num_occorrenze
num_occorrenze <- sum(tokens_full==parola_di_interesse)
cat("La parola", parola_di_interesse, " compare ", num_occorrenze, " volte nel testo")
# ordiniamo le parole per numero di occorrenze
freq_parole_ordinate <- freq_parole[order(-freq_parole$Freq),]
head(freq_parole_ordinate,10)
# esaminare una parola di interesse
parola_di_interesse <- "di"
tokens==parola_di_interesse
num_occorrenze <- freq_parole[freq_parole$tokens_full == parola_di_interesse,]
num_occorrenze
num_occorrenze$Freq
# guardiamo le parole piÃ¹ frequenti
head(freq_parole_ordinate,10)
#stopword
sw <- stopwords("it")
head(sw, 10)
class(sw)
tokens_clean <- tokens[!tokens_full %in% sw]
freq_parole <- as.data.frame(table(tokens_clean))
freq_parole_ordinate <- freq_parole[order(-freq_parole$Freq),]
head(freq_parole_ordinate,10)
ud_model <- udpipe_download_model(language = "italian", overwrite = F)
ud_it <- udpipe_load_model(ud_model)
# scriviamo una funzione per lemmatizzare
annotate_splits <- function(x, file) {
ud_model <- udpipe_load_model(file)
x <- as.data.table(udpipe_annotate(ud_model,
x = x$Text,
doc_id = x$id))
return(x)
}
# load parallel library future.apply
library(future.apply)
# Define cores to be used
ncores <- 4L
plan(multiprocess, workers = ncores)
plan(multisession, workers = ncores)
# split comments based on available cores
corpus_splitted <- split(my_data, seq(1, nrow(my_data), by = 500))
annotation <- future_lapply(corpus_splitted, annotate_splits, file = ud_model$file_model)
annotation <- rbindlist(annotation)
annotation
# split comments based on available cores
corpus_splitted <- split(my_data, seq(1, nrow(my_data), by = 500))
annotation <- future_lapply(corpus_splitted, annotate_splits, file = ud_model$file_model)
annotate_splits()
ud_model <- udpipe_download_model(language = "italian", overwrite = F)
ud_it <- udpipe_load_model(ud_model)
# scriviamo una funzione per lemmatizzare
annotate_splits <- function(x, file) {
ud_model <- udpipe_load_model(file)
x <- as.data.table(udpipe_annotate(ud_model,
x = x$Text,
doc_id = x$id))
return(x)
}
# load parallel library future.apply
library(future.apply)
# Define cores to be used
ncores <- 4L
plan(multisession, workers = ncores)
# split comments based on available cores
corpus_splitted <- split(my_data, seq(1, nrow(my_data), by = 500))
annotation <- future_lapply(corpus_splitted, annotate_splits, file = ud_model$file_model)
corpus_splitted
colnames(corpus_splitted)
corpus_splitted[0]
corpus_splitted[1]
colnames(corpus_splitted[1])
head(corpus_splitted[1])
head(corpus_splitted[1]$Text)
head(corpus_splitted$Text)
my_data
my_data$Text
# scriviamo una funzione per lemmatizzare
#prima inseriamo una colonna per identificare gli id dei testi nel dataframe
df$id <- se1(1:nrow(my_data))
# scriviamo una funzione per lemmatizzare
#prima inseriamo una colonna per identificare gli id dei testi nel dataframe
df$id <- seq(1:nrow(my_data))
#scriviamo la funzione
annotate_splits <- function(x, file) {
ud_model <- udpipe_load_model(file)
x <- as.data.table(udpipe_annotate(ud_model,
x = x$Text,
doc_id = x$id))
return(x)
}
# load parallel library future.apply
library(future.apply)
# Define cores to be used
ncores <- 4L
plan(multisession, workers = ncores)
# split comments based on available cores
corpus_splitted <- split(my_data, seq(1, nrow(my_data), by = 500))
annotation <- future_lapply(corpus_splitted, annotate_splits, file = ud_model$file_model)
# scriviamo una funzione per lemmatizzare
#prima inseriamo una colonna per identificare gli id dei testi nel dataframe
my_data$id <- seq(1:nrow(my_data))
#scriviamo la funzione
annotate_splits <- function(x, file) {
ud_model <- udpipe_load_model(file)
x <- as.data.table(udpipe_annotate(ud_model,
x = x$Text,
doc_id = x$id))
return(x)
}
# load parallel library future.apply
library(future.apply)
# Define cores to be used
ncores <- 4L
plan(multisession, workers = ncores)
# split comments based on available cores
corpus_splitted <- split(my_data, seq(1, nrow(my_data), by = 500))
annotation <- future_lapply(corpus_splitted, annotate_splits, file = ud_model$file_model)
annotation <- rbind(annotation)
annotation
annotation <- future_lapply(corpus_splitted, annotate_splits, file = ud_model$file_model)
annotation
annotation2 <- rbind(annotation)
annotation2
# importiamo il dataset
my_data <- read_excel("../materiali/dataset.xlsx")
my_data <- data.frame(my_data)
# dividere il corpus
corpus_splitted <- split(my_data, seq(1, nrow(my_data), by = 1000))
annotation <- future_lapply(corpus_splitted, annotate_splits, file = ud_model$file_model)
#prima inseriamo una colonna per identificare gli id dei testi nel dataframe
my_data$id <- seq(1:nrow(my_data))
# dividere il corpus
corpus_splitted <- split(my_data, seq(1, nrow(my_data), by = 1000))
annotation <- future_lapply(corpus_splitted, annotate_splits, file = ud_model$file_model)
head(annotation)
upos_df <- data.frame(table(annotation$upos))
ggplot(upos_df, aes(x=Var1, y=Freq)) + geom_histogram(stat='identity')
upos_Df
upos_df
annotation2 <- rbindlist(annotation)
head(annotation2)
upos_df <- data.frame(table(annotation$upos))
annotation <- rbindlist(annotation)
head(annotation)
upos_df <- data.frame(table(annotation$upos))
ggplot(upos_df, aes(x=Var1, y=Freq)) + geom_histogram(stat='identity')
annotation <- rbind(annotation)
head(annotation)
lemmi <- annotation$lemma[!annotation$lemma %in% sw]
freq_lemmi <- as.data.frame(table(lemmi))
freq_lemmi_ordinati <- freq_lemmi[order(-freq_lemmi$Freq),]
head(freq_lemmi_ordinati,10)
#tokenizzazione
tokens_full <- tokenize_words(testi)
tokens_full <- unlist(tokens_full)
tokens_clean <- tokens[!tokens_full %in% sw]
head(freq_lemmi_ordinati,10)
freq_lemmi_clean <- freq_lemmi_ordinati[!freq_lemmi_ordinati$lemmi in sw]
freq_lemmi_clean <- freq_lemmi_ordinati[!freq_lemmi_ordinati$lemmi %in% sw]
freq_lemmi_clean <- freq_lemmi_ordinati[!freq_lemmi_ordinati$lemmi %in% sw,]
freq_lemmi_clean
freq_lemmi_clean[freq_lemmi_clean$lemmi == 'salvini']
freq_lemmi_clean[freq_lemmi_clean$lemmi == 'salvino']
freq_lemmi_clean[freq_lemmi_clean$lemmi == 'salvare']
freq_lemmi_clean[freq_lemmi_clean$lemmi == 'salvare',]
freq_lemmi_clean[freq_lemmi_clean$lemmi == 'salv',]
head(freq_lemmi_clean,100)
tolower(my_data$Text)
#testo in minuscolo
my_data$Text[10]
#testo in minuscolo
test <- my_data$Text[10]
#testo in minuscolo
test <- my_data$Text[5]
print(test)
#testo in minuscolo
test <- my_data$Text[12]
print(test)
print(tolower(test))
#ricarichiamo il nostro file di testo
my_data <- read_excel("../materiali/dataset.xlsx")
my_data <- data.frame(my_data)
my_data$Style[my_data$Style == "positivr"] = "positive"
# quali classi di sono presenti nel dataset?
my_data$Style
table(my_data$Style)
#ricarichiamo il nostro file di testo
my_data <- read_excel("../materiali/dataset.xlsx")
my_data <- data.frame(my_data)
# quali classi di sono presenti nel dataset?
my_data$Style
table(my_data$Style)
total_tokens <- length(tokens_full)
as.data.frame(table(tokens_full))
as.data.frame(table(factor(tokens_full, levels=unique(tokens_full))))
as.data.frame(table(tokens_full))
as.data.frame(table(tokens_full))
count_total_tokens <- as.data.frame(table(tokens_full))
plot(count_total_tokens$Freq[count_total_tokens$Freq <= 500])
count_total_tokens
# **estraiamo i token per una data polaritÃ **
testi_classe <- my_data[my_data$Style=='negative',]$Text
tokens_classe <- unlist(tokenize_words(testi_classe))
tokens_classe <- tokens_classe[!tokens_classe %in% sw]
table(
#factor ci serve ad avere anche i token che compaiono con occorrenza = 0 in una data classe
factor(
tokens_classe, levels = unique(tokens_full)
)
#tokens_classe
)
table(
#factor ci serve ad avere anche i token che compaiono con occorrenza = 0 in una data classe
tokens_classe, levels = unique(tokens_full)
#tokens_classe
)
factor()  #factor ci serve ad avere anche i token che compaiono con occorrenza = 0 in una data classe
factor(  #factor ci serve ad avere anche i token che compaiono con occorrenza = 0 in una data classe
tokens_classe, levels = unique(tokens_full)
#tokens_classe
)
# scriviamo una funzione per estrare i token presenti nei post con una certa polarity
get_frequencies <- function(polarity){
testi_classe <- my_data[my_data$Style==polarity,]$Text
tokens_classe <- unlist(tokenize_words(testi_classe))
tokens_classe <- tokens_classe[!tokens_classe %in% sw]
dataframe_tf <- as.data.frame(
table(
#factor ci serve ad avere anche i token che compaiono con occorrenza = 0 in una data classe
factor(
tokens_classe, levels = unique(tokens_full)
)
#tokens_classe
)
)
dataframe_tf['tf'] <- dataframe_tf$Freq/length(tokens_classe)
dataframe_tf['classe'] <- c(polarity)
dataframe_tf <- rename(dataframe_tf, 'tokens'=Var1)
return(dataframe_tf)
}
dataframe_tf_pos <- get_frequencies("positive")
dataframe_tf_neg <- get_frequencies("negative")
dataframe_tf_na <- get_frequencies("not applicable")
dataframe_tf_full <- bind_rows(
dataframe_tf_pos,
dataframe_tf_neg,
dataframe_tf_na,
.id = "column_label")
head(dataframe_tf_full)
parola <- 'salvini'
dataframe_parola <- dataframe_tf_full[dataframe_tf_full$tokens==parola,]
dataframe_parola
ggplot(data=dataframe_parola) +
geom_bar(mapping=aes(x=classe, y=tf), stat='identity')
# visualizzare parole per frequenza data una classe
classe_specifica <- 'positive'
dataframe_classe <- dataframe_tf_full[dataframe_tf_full$classe==classe_specifica,]
dataframe_classe
colnames(dataframe_classe)
nrow(dataframe_classe)
dataframe_classe_ordinato <- head(dataframe_classe[order(-dataframe_classe$tf),],10)
dataframe_classe_ordinato
ggplot(data=dataframe_classe_ordinato) +
geom_bar(mapping=aes(x=reorder(tokens,-Freq) , y=Freq), stat='identity')
#ricarichiamo il nostro file di testo
my_data <- read_excel("../materiali/dataset.xlsx")
my_data <- data.frame(my_data)
sw <- stopwords("it")
testi <- my_data$Text
style <- my_data$Style
#tokenizzazione
tokens_full <- tokenize_words(testi)
tokens_full <- tokens_full[!tokens_full %in% sw]
# quali classi di sono presenti nel dataset?
table(my_data$Style)
my_data$Style[my_data$Style == "positivr"] = "positive"
my_data$Style <- ifelse(is.na(my_data$Style),
"not applicable",
my_data$Style)
#ricarichiamo il nostro file di testo
my_data <- read_excel("../materiali/dataset.xlsx")
my_data <- data.frame(my_data)
sw <- stopwords("it")
testi <- my_data$Text
style <- my_data$Style
#tokenizzazione ed eliminazione stop word
tokens_full <- tokenize_words(testi)
tokens_full <- tokens_full[!tokens_full %in% sw]
my_data$Style[my_data$Style == "positivr"] = "positive"
my_data$Style <- ifelse(is.na(my_data$Style),
"not applicable",
my_data$Style)
testi_positive <- my_data[my_data$Style=="positive",]$Text
tokens_positive <- unlist(tokenize_words(testi_positive))
tokens_positive <- tokens_positive[!tokens_positive %in% sw]
wordcloud(words = tokens_positive, freq = table(tokens_positive)$Freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
library(tidyverse)
library(ggplot2)
library(tokenizers)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(udpipe)
library(stopwords)
library(R.utils)
library(tm)
set.seed(1234)
wordcloud(words = tokens_positive, freq = table(tokens_positive)$Freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = tokens_positive, freq = as.data.frame(table(tokens_positive))$Freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = tokens_positive, freq = as.data.frame(table(tokens_positive))$Freq, min.freq = 1,
max.words=200, random.order=FALSE, size=1.6,
colors=brewer.pal(8, "Dark2"))
my_data$style_num <- ifelse(
my_data$Style=='positive',
1,
0
)
sw <- stopwords("it")
ggplot(my_data, aes(x=id, fill=style_num))
my_data$id <- seq(1:nrow(my_data))
ggplot(my_data, aes(x=id, fill=style_num))
ggplot(my_data, aes(x=id, fill=style_num))+
geom_tile(color="black", size=0.5)
ggplot(my_data, aes(x=id, y=1, fill=style_num))+
geom_tile(color="black", size=0.5)
ggplot(my_data, aes(x=id, y=style_num, fill=style_num))+
geom_tile(color="black", size=0.5)
my_data$style_num <- ifelse(
my_data$Style=='positive',
1,
-1
)
ggplot(my_data, aes(x=id, y=style_num))+
geom_bar()
ggplot(my_data, aes(y=style_num))+
geom_bar()
ggplot(my_data, aes(y=style_num))+
geom_line()
ggplot(my_data, aes(x=id, y=style_num))+
geom_line()
my_data$style_num
ggplot(my_data, aes(x=id))+
geom_line(y=syle_num)
ggplot(my_data, aes(x=id))+
geom_line(y=style_num)
ggplot(my_data, aes(x=id))+
geom_line(aes(y=style_num))
ggplot(my_data, aes())+
geom_line(aes(y=style_num))
ggplot(my_data, aes())+
geom_line(aes(x=id, y=style_num))
my_data$id
ggplot(my_data, aes())+
geom_bar(aes(x=id))
ggplot(my_data, aes())+
geom_bar(aes(x=id, y=style_num), stat='identity')
ggplot(my_data, aes())+
geom_line(aes(x=id, y=style_num), stat='identity')
ggplot(my_data, aes())+
geom_line(aes(y=style_num), stat='identity')
ggplot(my_data, aes(x=id))+
geom_line(aes(y=style_num), stat='identity')
ggplot(my_data, aes(1, id)+
ggplot(my_data, aes(1, id))+
geom_line(aes(y=style_num), stat='identity')
ggplot(my_data, aes(1, id))+
geom_line(aes(y=style_num), stat='identity')
ggplot(my_data, aes(1, id))
ggplot(my_data, aes(1, id))+
geom_tile(aes(fill=style_num))
ggplot(my_data, aes(id,1))+
geom_tile(aes(fill=style_num))
ggplot(my_data, aes(id,0.5))+
geom_tile(aes(fill=style_num))
ggplot(my_data)+
geom_bar(aes(y=Style))
ggplot(my_data)+
geom_bar(aes(y=Style, fill=Style))
#valutazioni dei risultati
#correlazione tra la sentiment calcolata e le statistiche del dataset
calcolare_sentiment_nrc <- function(frase_input){
tokens <- unlist(tokenize_words(frase_input))
tokens_clean <- tokens[!tokens %in% sw]
sentiment_values <- nrc_table[is.element(nrc_table$`Italian-it`, tokens_clean),]$Valence
return(mean(sentiment_values))
}
sentiment_values <- unlist(lapply(testi, calcolare_sentiment_nrc))
sentiment_values[is.na(sentiment_values)] <- 0
sentiment_values_dataframe <- data.frame(
testi, sentiment_values
)
head(sentiment_values_dataframe)
sentiment_values_dataframe  <- sentiment_values_dataframe[
order(-sentiment_values_dataframe$sentiment_values)
,]
sentiment_values_dataframe$index <- seq(
from=1,
to=nrow(sentiment_values_dataframe),
by=1)
sentiment_values_dataframe <- data.frame(
sentiment_values, my_data$Style
)
ggplot(sentiment_values_dataframe, aes(x=style, y=sentiment_values)) +
geom_point()
ggplot(sentiment_values_dataframe, aes=(y=sentiment_values_dataframe$sentiment_values))+
mark_bar
ggplot(sentiment_values_dataframe, aes=(y=sentiment_values_dataframe$sentiment_values))+
geom_bar
ggplot(sentiment_values_dataframe, aes=(y=sentiment_values_dataframe$sentiment_values))+
geom_bar()
sentime
sentiment_values_dataframe
ggplot(sentiment_values_dataframe, aes=(y=sentiment_values_dataframe$sentiment_values))+
geom_bar(x=1:nrow(sentiment_values_dataframe))
geom_bar(x=seq(1:nrow(sentiment_values_dataframe))
ggplot(sentiment_values_dataframe, aes=(y=sentiment_values_dataframe$sentiment_values))+
geom_bar(x=seq(1:nrow(sentiment_values_dataframe)))
ggplot(sentiment_values_dataframe, aes=(y=sentiment_values_dataframe$sentiment_values))+
geom_bar(x=seq(1:nrow(sentiment_values_dataframe)+1))
ggplot(sentiment_values_dataframe, aes=(y=sentiment_values_dataframe$sentiment_values))+
geom_bar(x=seq(1:nrow(sentiment_values_dataframe)+1), stat='identity')
ggplot(sentiment_values_dataframe)+
geom_bar(x=seq(1:nrow(sentiment_values_dataframe)+1, y=sentiment_values_dataframe$sentiment_values), stat='identity')
ggplot(sentiment_values_dataframe)+
geom_bar(x=seq(1:nrow(sentiment_values_dataframe)+1),
y=sentiment_values_dataframe$sentiment_values), stat='identity')
ggplot(sentiment_values_dataframe)+
geom_bar(x=seq(1:nrow(sentiment_values_dataframe)+1),
y=sentiment_values_dataframe$sentiment_values), stat='identity')
ggplot(sentiment_values_dataframe)+
geom_bar(aes(x=seq(1:nrow(sentiment_values_dataframe)+1),
y=sentiment_values_dataframe$sentiment_values), stat='identity')
ggplot(sentiment_values_dataframe)+
geom_bar(aes(x=seq(1:nrow(sentiment_values_dataframe)+1),
y=sentiment_values_dataframe$sentiment_values,
fill=sentiment_values_dataframe$sentiment_values,), stat='identity')
ggplot(sentiment_values_dataframe)+
geom_bar(aes(x=seq(1:nrow(sentiment_values_dataframe)+1),
y=sentiment_values,
fill=sentiment_values), stat='identity')
ggplot(sentiment_values_dataframe)+
geom_bar(aes(x=seq(1:nrow(sentiment_values_dataframe)+1),
y=sentiment_values,
fill=sentiment_values,
colour='red'), stat='identity')
ggplot(sentiment_values_dataframe)+
geom_bar(aes(x=seq(1:nrow(sentiment_values_dataframe)+1),
y=sentiment_values,
fill=sentiment_values,
), stat='identity')
ggplot(sentiment_values_dataframe)+
geom_bar(aes(x=seq(1:nrow(sentiment_values_dataframe)+1),
y=sentiment_values,
color=sentiment_values,
), stat='identity')
