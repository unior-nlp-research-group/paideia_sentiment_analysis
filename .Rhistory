"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato$Style[my_data$Style == "positivr"] <- "positive"
data_annotato$Style <- ifelse(is.na(my_data$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato[my_data$Style == "positivr"]$Style <- "positive"
data_annotato$Style <- ifelse(is.na(my_data$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
data_annotato[my_data$Style == "positivr",]$Style <- "positive"
data_annotato$Style <- ifelse(is.na(my_data$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato$Style[my_data$Style == "positivr"] <- "positive"
style <- data_annotato$Style
table(style)
data_annotato$Style <- ifelse(is.na(my_data$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato$Style[my_data$Style == "positivr"] <- "positive"
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato$Style[data_annotato$Style == "positivr"] <- "positive"
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato$Style[data_annotato$Style == "positivr"] <- "positive"
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
data_annotato$Style)
style <- data_annotato$Style
table(style)
labels_train <- style[1:3000]
labels_test <- style[3000:3792]
prop.table(table(labels_train))
train_dtm
train_split
train_index = sample(3792, 3000)
data_annotato
head(data_annotato)
train_sentences <- lemmatized_texts[train_index]
test_sentences <- lemmatized_texts[-train_index]
train_corpus <- VCorpus(VectorSource(train_sentences))
test_corpus <- VCorpus(VectorSource(test_sentences))
train_dtm <- DocumentTermMatrix(train_corpus)
test_dtm <- DocumentTermMatrix(test_corpus)
tm::inspect(test_dtm)
train_labels <- style[train_index]
test_labels <- style[-train_index]
head(train_labels)
classifier <- naiveBayes(as.matrix(train_dtm), train_labels)
predict(classifier, "Secondo me Salvini rappresenta la sola speranza per questo paese")
predict(classifier, "Secondo me Salvini rappresenterebbe un pessimo premier")
predict(classifier, "Secondo me salvino rappresenterebbe un pessimo premier")
predict(classifier, "Secondo me salvino rappresenterebbe un pessimo premier orrendo")
wordcloud(train_sentences, max.words=40, scale=c(3,0.5))
wordcloud(lemmatized_texts[train_index], max.words=40, scale=c(3,0.5))
class(train_sentences)
wordcloud(train_corpus, max.words=40, scale=c(3,0.5))
pos <- subset(data_annotato, Style == 'positive')
wordcloud(pos$Text, max.words=40, scale=c(3,0.5))
wordcloud(pos$Text, max.words=40, scale=c(3,10))
wordcloud(pos$Text, max.words=40, scale=c(3,0.1))
wordcloud(pos$Text, max.words=50, scale=c(3,0.1))
pos_ids <- data_annotato$id[data_annotato$Style == 'positve']
pos_ids <- data_annotato$id[data_annotato$Style == 'positive']
wordcloud(lemmatized_texts[pos_ids], max.words=50, scale=c(3,0.1))
lemmatied_texts[pos_ids]
lemmatized_texts[pos_ids]
pos_texts <- lemmatized_texts[pos_ids]
wordcloud(pos_texts, max.words=50, scale=c(3,0.1))
pos_texts <- VCorpus(VectorSource(lemmatized_texts[pos_ids]))
wordcloud(pos_texts, max.words=50, scale=c(3,0.1))
wordcloud(pos_texts, max.words=50, scale=c(2,0.1))
wordcloud(pos_texts, max.words=50, scale=c(3,0.1))
wordcloud(pos_texts, max.words=50, scale=c(3,0.5))
wordcloud(pos_texts, max.words=50, scale=c(4,0.5))
neg_ids <- data_annotato$id[data_annotato$Style == 'negative']
neg_texts <- VCorpus(VectorSource(lemmatized_texts[neg_ids]))
wordcloud(neg_texts, max.words=50, scale=c(4,0.5))
wordcloud(pos_texts, max.words=50, scale=c(4,0.5))
wordcloud(neg_texts, max.words=50, scale=c(4,0.5))
predict(classifier, "Secondo me Salvini rappresenta la sola speranza per questo paese")
predict(classifier, "Secondo me Salvini rappresenterebbe un pessimo premier orrendo")
librerie <- c('tidyverse',
'tokenizers',
'udpipe',
'tm',
'stopwords',
'R.utils',
"tidytext",
'textdata',
"readxl",
"parallel",
"future.apply",
"irr",
"RTextTools",
"e1071",
"rlang",
"hardhat",
"recipes",
"caret"
)
library(tidyverse)
library(tokenizers)
library(udpipe)
library(stopwords)
library(R.utils)
library(tm)
library(parallel)
library(readxl)
library(data.table)
# importiamo il dataset
my_data <- read_excel("./materiali/dataset.xlsx")
my_data <- data.frame(my_data)
clean_text <- function(text){
text <- tolower(text)
text <- gsub(".", " ", text, fixed=TRUE)
text <- gsub(":", " ", text, fixed=TRUE)
text <- gsub("?", " ", text, fixed=TRUE)
text <- gsub("!", " ", text, fixed=TRUE)
text <- gsub("; ", " ", text, fixed=TRUE)
text <- gsub(", ", " ", text, fixed=TRUE)
text <- gsub("\ `", " ", text, fixed=TRUE)
text <- gsub("\n", " ", text, fixed=TRUE)
text <- gsub("\r", " ", text, fixed=TRUE)
return(text)
}
my_data$clean_text <- clean_text(my_data$Text)
# analisi sintattica
ud_it <- udpipe_load_model(file='./materiali/italian-isdt-ud-2.5-191206.udpipe')
# scriviamo una funzione per estrarre info sintattiche
annotate_splits <- function(x) {
ud_model <- ud_it
x <- as.data.table(udpipe_annotate(ud_model,
x = x$clean_text,
doc_id = x$id))
return(x)
}
# load parallel library future.apply
library(future.apply)
# numero di core da utilizzare
ncores <- 3L
plan(multiprocess, workers = ncores)
#prima inseriamo una colonna per identificare gli id dei testi nel dataframe
my_data$id <- seq(1:nrow(my_data))
# dividere il corpus
corpus_splitted <- split(my_data, seq(1, nrow(my_data), by = 1000))
annotation <- future_lapply(corpus_splitted, annotate_splits)
annotation <- rbindlist(annotation)
head(annotation)
write.csv(annotation, 'annotazioni-sintattiche.csv')
install.packages(jsonlite)
library(jsonlite)
install.packages("jsonlite")
install.packages("jsonlite")
library(jsonlite)
files <- list.files(path='./materiali/annotazioni_amazon/',
pattern="utente_paideia*",
full.names=TRUE)
files
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
library(dplyr)
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
class(dfs[[1]])
dfs <- lapply(files, readfiles)
files <- list.files(path='./materiali/annotazioni_amazon',
pattern="utente_paideia*",
full.names=TRUE)
files
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
files <- list.files(path='./materiali/annotazioni_amazon',
full.names=TRUE)
files
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
files <- list.files(path='./materiali/annotazioni_amazon',
pattern='utente_paideia*'
full.names=TRUE)
files
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
class(dfs[[1]])
files <- list.files(path='./materiali/annotazioni_amazon',
pattern='utente_paideia*',
full.names=TRUE)
files
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
class(dfs[[1]])
colnames(dfs[[1]])
length(dfs)
dataframe_id1_annotatore1 <- dfs[[1]]$entities.id1
dataframe_id1_annotatore1
dataframe_id1_annotatore1 <- dfs[[1]]$entities.id1
dataframe_id2_annotatore1 <- dfs[[2]]$entities.id1
dataframe_id3_annotatore1 <- dfs[[3]]$entities.id1
dataframe_id4_annotatore1 <- dfs[[4]]$entities.id1
dataframe_id5_annotatore1 <- dfs[[5]]$entities.id1
dataframe_id6_annotatore1 <- dfs[[6]]$entities.id1
dataframe_id7_annotatore1 <- dfs[[7]]$entities.id1
dataframe_id8_annotatore1 <- dfs[[8]]$entities.id1
dataframe_id9_annotatore1 <- dfs[[9]]$entities.id1
dataframe_id10_annotatore1 <- dfs[[10]]$entities.id1
dataframe_id11_annotatore1 <- dfs[[11]]$entities.id1
dataframe_id12_annotatore1 <- dfs[[12]]$entities.id1
dataframe_id13_annotatore1 <- dfs[[13]]$entities.id1
kappam.fleiss(c(
dataframe_id1_annotatore1,
dataframe_id1_annotatore1
dataframe_id1_annotatore1
))
dataframe_id1_annotatore1# analisi IAA
kappam.fleiss(c(
dataframe_id1_annotatore1,
dataframe_id2_annotatore1,
dataframe_id3_annotatore1,
dataframe_id4_annotatore1,
dataframe_id5_annotatore1,dataframe_id6_annotatore1,
dataframe_id8_annotatore1,dataframe_id7_annotatore1,
dataframe_id9_annotatore1,dataframe_id10_annotatore1,
dataframe_id11_annotatore1, dataframe_id12_annotatore1
dataframe_id1_annotatore1,dataframe_id1_annotatore1
))
kappam.fleiss(c(
dataframe_id1_annotatore1,
dataframe_id2_annotatore1,
dataframe_id3_annotatore1,
dataframe_id4_annotatore1,
dataframe_id5_annotatore1,dataframe_id6_annotatore1,
dataframe_id8_annotatore1,dataframe_id7_annotatore1,
dataframe_id9_annotatore1,dataframe_id10_annotatore1,
dataframe_id11_annotatore1,dataframe_id12_annotatore1)
))
kappam.fleiss(c(
dataframe_id1_annotatore1,
dataframe_id2_annotatore1,
dataframe_id3_annotatore1,
dataframe_id4_annotatore1,
dataframe_id5_annotatore1,dataframe_id6_annotatore1,
dataframe_id8_annotatore1,dataframe_id7_annotatore1,
dataframe_id9_annotatore1,dataframe_id10_annotatore1,
dataframe_id11_annotatore1,dataframe_id12_annotatore1))
install.packages(kappam)
install.packages("kappam")
library(kappam)
library(irr)
kappam.fleiss(c(
dataframe_id1_annotatore1,
dataframe_id2_annotatore1,
dataframe_id3_annotatore1,
dataframe_id4_annotatore1,
dataframe_id5_annotatore1,dataframe_id6_annotatore1,
dataframe_id8_annotatore1,dataframe_id7_annotatore1,
dataframe_id9_annotatore1,dataframe_id10_annotatore1,
dataframe_id11_annotatore1,dataframe_id12_annotatore1))
dataframe_id1_annotatore1 <- dfs[[1]]$entities.id1[dfs[[1]]$entities.id1==NA] <- 'None'
dataframe_id1_annotatore1 <- dfs[[1]]$entities.id1[dfs[[1]]$entities.id1==NA] <- 'None'
dataframe_id2_annotatore1 <- dfs[[2]]$entities.id1[dfs[[2]]$entities.id1==NA] <- 'None'
dataframe_id3_annotatore1 <- dfs[[3]]$entities.id1[dfs[[3]]$entities.id1==NA] <- 'None'
dataframe_id4_annotatore1 <- dfs[[4]]$entities.id1[dfs[[4]]$entities.id1==NA] <- 'None'
dataframe_id5_annotatore1 <- dfs[[5]]$entities.id1[dfs[[5]]$entities.id1==NA] <- 'None'
dataframe_id6_annotatore1 <- dfs[[6]]$entities.id1[dfs[[6]]$entities.id1==NA] <- 'None'
dataframe_id6_annotatore1
dataframe_id7_annotatore1 <- dfs[[7]]$entities.id1[dfs[[7]]$entities.id1==NA] <- 'None'
dataframe_id7_annotatore1
dataframe_id1_annotatore1
dfs[[1]]$entities.id1==NA
is.na(dfs[[1]]$entities.id1)
dataframe_id1_annotatore1 <- dfs[[1]]$entities.id1[is.na(dfs[[1]]$entities.id1)] <- 'None'
dataframe_id1_annotatore1
dfs[[1]]$entities.id1
files <- list.files(path='./materiali/annotazioni_amazon',
pattern='utente_paideia*',
full.names=TRUE)
files
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
dataframe_id6_annotatore1 <- dfs[[6]]$entities.id1
dataframe_id1_annotatore1 <- dfs[[1]]$entities.id1
dataframe_id2_annotatore1 <- dfs[[2]]$entities.id1
dataframe_id4_annotatore1 <- dfs[[4]]$entities.id1
dataframe_id5_annotatore1 <- dfs[[5]]$entities.id1
dataframe_id7_annotatore1 <- dfs[[7]]$entities.id1
dataframe_id1_annotatore1[is.na(dataframe_id1_annotatore1)] <- "None"
dataframe_id1_annotatore1
dataframe_id1_annotatore1[is.na(dataframe_id1_annotatore1)] <- "None"
dataframe_id2_annotatore1[is.na(dataframe_id2_annotatore1)] <- "None"
dataframe_id4_annotatore1[is.na(dataframe_id4_annotatore1)] <- "None"
dataframe_id5_annotatore1[is.na(dataframe_id5_annotatore1)] <- "None"
dataframe_id7_annotatore1[is.na(dataframe_id7_annotatore1)] <- "None"
kappam.fleiss(c(
dataframe_id1_annotatore1,
dataframe_id2_annotatore1,
dataframe_id4_annotatore1,
dataframe_id5_annotatore1,
dataframe_id7_annotatore1))
dataframe_id1_annotatore1
#importiamo le librerie che useremo in questa giornata
library(tidyverse)
library(ggplot2)
library(tokenizers)
library(stopwords)
library(tidytext)
library(udpipe)
library(tm)
library(irr)
library(readxl)
library(data.table)
Reviews_Dooge_X10 <- read.csv("~/Progetti/paideia_sentiment_analysis/materiali/Reviews_Dooge_X10.csv", header=FALSE)
View(Reviews_Dooge_X10)
#ricarichiamo il nostro dataset
my_data <- read.csv("~/Progetti/paideia_sentiment_analysis/materiali/Reviews_Dooge_X10.csv", header=FALSE)
my_data
my_data <- data.frame(my_data)
class(my_data)
colnames(my_data)
#ricarichiamo il nostro dataset
my_data <- read.csv("~/Progetti/paideia_sentiment_analysis/materiali/Reviews_Dooge_X10.csv",
header=FALSE)
my_data <- data.frame(my_data)
colnames(my_data)
head(my_data$V1)
head(my_data$V2)
head(my_data$V3)
tokens <- tokenize_words(my_data$V3)
tokens[1]
tokens <- unlist(tokens)
head(tokens)
summary(tokens)
table(tokens)
tokens_df <- data.frame(table(tokens))
head(tokens_df)
head(tokens)
table(tokens)
library(tidyverse)
library(ggplot2)
library(tokenizers)
library(stopwords)
library(tidytext)
library(udpipe)
library(tm)
library(irr)
library(readxl)
library(data.table)
head(tokens_df)
length(tokens)
head(tokens_df)
sw <- stopwords("it")
sw
stopwords("en")
sw <- stopwords("it")
sw
tokens[tokens!=sw]
tokens %in% sw
tokens != "None"
tokens <- tokens[!tokens %in% sw]
head(tokens)
tokens <- tokens[!tokens %in% sw]
tokens_df <- data.frame(table(tokens))
head(tokens_df)
tokens_df[order(tokens_df$Freq)]
tokens_df[order(tokens_df$Freq),]
colnames(tokens_df)
tokens_df$Freq
order(tokens_df$Freq)
head(tokens_df[order(tokens_df$Freq),],10)
head(tokens_df[!order(tokens_df$Freq),],10)
head(tokens_df[order(-tokens_df$Freq),],10)
nrow(my_data)
## 1. dividiamo il dataset per rating
colnames(my_data)
head(my_data$1)
head(my_data$V1)
recensioni_5 <- my_data$V3[my_data$V1==5]
head(recensioni_5)
my_data$V1[1]
sum(my_data$V1==5)
table(my_data$V1)
recensioni_5 <- my_data$V3[my_data$V1==5]
recensioni_4 <- my_data$V3[my_data$V1==4]
recensioni_3 <- my_data$V3[my_data$V1==3]
recensioni_2 <- my_data$V3[my_data$V1==2]
recensioni_1 <- my_data$V3[my_data$V1==1]
class(recensione_5)
class(recensioni_5)
tokens <- tokenize_words(recensioni_5)
tokens[1]
tokens_5 <- unlist(tokenize_words(recensioni_5))
tokens_4 <- unlist(tokenize_words(recensioni_4))
tokens_3 <- unlist(tokenize_words(recensioni_3))
tokens_2 <- unlist(tokenize_words(recensioni_2))
tokens_1 <- unlist(tokenize_words(recensioni_1))
freq_5 <- data.frame(table(tokens_5))
freq_4 <- data.frame(table(tokens_4))
freq_3 <- data.frame(table(tokens_3))
freq_2 <- data.frame(table(tokens_2))
freq_1 <- data.frame(table(tokens_1))
head(freq_4)
freq_5 <- freq_5[order(-freq_5$Freq)]
freq_5 <- freq_5[order(-freq_5$Freq),]
freq_5 <- freq_5[order(-freq_5$Freq),]
freq_4 <- freq_5[order(-freq_4$Freq),]
freq_3 <- freq_5[order(-freq_3$Freq),]
freq_2 <- freq_5[order(-freq_2$Freq),]
freq_1 <- freq_5[order(-freq_1$Freq),]
freq_5 <- freq_5[order(-freq_5$Freq)]
head(freq_1)
freq_5 <- freq_5[order(-freq_5$Freq),]
freq_4 <- freq_4[order(-freq_4$Freq),]
freq_3 <- freq_3[order(-freq_3$Freq),]
freq_2 <- freq_2[order(-freq_2$Freq),]
freq_1 <- freq_1[order(-freq_1$Freq),]
head(freq_1)
colnames(freq_5)
freq_5 <- freq_5[!freq_5$tokens_5 %in% sw]
freq_5 <- freq_5[!freq_5$tokens_5 %in% sw,]
freq_5
head(freq_5)
freq_1 <- freq_1[!freq_1$tokens_1 %in% sw,]
head(freq_1)
freq_1 <- freq_1[!freq_1$tokens_1 %in% sw,]
head(freq_1)
head(freq_1)
freq_1 <- freq_1[order(-freq_1$Freq),]
head(freq_1)
head(freq_2)
colnames(freq_1)
freq_1 <- data.frame(table(tokens_1))
colnames(freq_1)
freq_5 <- data.frame(table(tokens_5))
freq_4 <- data.frame(table(tokens_4))
freq_3 <- data.frame(table(tokens_3))
freq_2 <- data.frame(table(tokens_2))
freq_1 <- data.frame(table(tokens_1))
freq_5 <- freq_5[order(-freq_5$Freq),]
freq_4 <- freq_4[order(-freq_4$Freq),]
freq_3 <- freq_3[order(-freq_3$Freq),]
freq_2 <- freq_2[order(-freq_2$Freq),]
freq_1 <- freq_1[order(-freq_1$Freq),]
colnames(freq_1)
freq_1 <- freq_1[!freq_1$tokens_1 %in% sw,]
head(freq_1)
"dopo" %in% sw
head(freq_3)
freq_3 <- freq_3[!freq_3$tokens_3 %in% sw,]
freq_3
recensioni_3 <- my_data$V3[my_data$V1==3]
tokens_3 <- unlist(tokenize_words(recensioni_3))
freq_3 <- data.frame(table(tokens_3))
freq_3 <- freq_3[order(-freq_3$Freq),]
freq_3 <- freq_3[!freq_3$tokens_3 %in% sw,]
freq_3
head(freq_3)
