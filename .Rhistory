"not applicable",
data_annotato$Style)
style <- data_annotato$Style
#importare lessico esterno
nrc_table <- as.data.frame(
fread('./materiali/sentix'
)
)
library(data.table)
#importare lessico esterno
nrc_table <- as.data.frame(
fread('./materiali/sentix'
)
)
#importare lessico esterno
lessico <- as.data.frame(
fread('./materiali/sentix'
)
)
head(lessico)
frase <- c("Sono molto fiducioso per il futuro, anche se mi sento un po' preoccupato a causa degli ultimi avvenimenti")
tokens_frase <- tokenize_words(frase)
tokens_frase <- unlist(tokens_frase)
tokens_frase <- tokens_frase[!tokens_frase %in% sw]
nrc_table[is.element(lessico$V1, tokens_frase),]
lessico[is.element(lessico$V1, tokens_frase),]
sentiment_values <- lessico[is.element(lessico$V1, tokens_frase),]$V6
sentiment_values
lessico[is.element(lessico$V1, tokens_frase),]
sentiment_values <- lessico[is.element(lessico$V1, tokens_frase),]$V6
sentiment_values
calcolare_sentiment <- function(frase_input){
tokens <- unlist(tokenize_words(frase_input))
tokens_clean <- tokens[!tokens %in% sw]
sentiment_values <- lessico[is.element(lessico$V1, tokens_clean),]$V6
return(mean(sentiment_values))
}
sentiment_values <- unlist(lapply(lemmatized_texts, calcolare_sentiment))
head(sentiment_values)
is.na(sentiment_values)
# creare split
set.seed(12)
train_index = sample(nrow(sentiment_values_dataframe), 3000)
library(tidyverse)
library(ggplot2)
library(tokenizers)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(udpipe)
library(stopwords)
library(R.utils)
library(data.table)
library(tm)
library(readxl)
library(RTextTools)
library(e1071)
library(caret)
sw <- stopwords("it")
#ricarichiamo il nostro file annotato sintatticamente
data <- read_csv('./materiali/annotazioni-sintattiche.csv')
class(data)
colnames(data)
#data <- as.data.frame(data)
testi_lemmatizzati <- c()
for (i in unique(data$doc_id)) {
tokens <- data[data$doc_id==i,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
testi_lemmatizzati <- c(testi_lemmatizzati, list(tokens))
}
testi_lemmatizzati[1]
data_annotato <- data.frame(data_annotato)
data_annotato <- read_excel("./materiali/dataset.xlsx")
data_annotato <- data.frame(data_annotato)
testi <- data_annotato$Text
data_annotato$Style[data_annotato$Style == "positivr"] <- "positive"
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
data_annotato$Style)
style <- data_annotato$Style
#importare lessico esterno
lessico <- as.data.frame(
fread('./materiali/sentix'
)
)
head(lessico)
frase <- c("Sono molto fiducioso per il futuro, anche se mi sento un po' preoccupato a causa degli ultimi avvenimenti")
tokens_frase <- tokenize_words(frase)
tokens_frase <- unlist(tokens_frase)
tokens_frase <- tokens_frase[!tokens_frase %in% sw]
lessico[is.element(lessico$V1, tokens_frase),]
sentiment_values <- lessico[is.element(lessico$V1, tokens_frase),]$V6
sentiment_values
mean(sentiment_values)
calcolare_sentiment <- function(frase_input){
tokens <- unlist(tokenize_words(frase_input))
tokens_clean <- tokens[!tokens %in% sw]
sentiment_values <- lessico[is.element(lessico$V1, tokens_clean),]$V6
return(mean(sentiment_values))
}
sentiment_values <- unlist(lapply(lemmatized_texts, calcolare_sentiment))
sentiment_values <- unlist(lapply(testi_lemmatizzati, calcolare_sentiment))
head(sentiment_values)
hist(head(sentiment_values))
hist(sentiment_values)
hist(table(sentiment_values))
hist(sentiment_values)
sentiment_values_dataframe <- data.frame(
testi, sentiment_values
)
head(sentiment_values_dataframe)
sentiment_values_dataframe$index <- seq(
from=1,
to=nrow(sentiment_values_dataframe),
by=1)
colnames(sentiment_values_dataframe)
sentiment_values_dataframe$polarity <- ifelse(
sentiment_values_dataframe$sentiment_values >= 0.7,
"positivo",
ifelse(sentiment_values_dataframe$sentiment_values <= 0.5,
"negativo",
"neutro")
)
table(sentiment_values_dataframe$polarity)
# creare split
set.seed(12)
train_index = sample(nrow(sentiment_values_dataframe), 3000)
train_sentences <- lemmatized_texts[train_index]
length(train_index)
train_sentences <- testi_lemmatizzati[train_index]
?VCorpus
?VectorSource
head(VectorSource(train_sentences))
head(VCorpus(train_sentences))
test_sentences <- testi_lemmatizzati[-train_index]
test_corpus <- VCorpus(VectorSource(test_sentences))
train_dtm <- DocumentTermMatrix(train_corpus)
test_dtm <- DocumentTermMatrix(test_corpus)
train_corpus <- VCorpus(VectorSource(train_sentences))
test_sentences <- testi_lemmatizzati[-train_index]
test_corpus <- VCorpus(VectorSource(test_sentences))
train_dtm <- DocumentTermMatrix(train_corpus)
test_dtm <- DocumentTermMatrix(test_corpus)
tm::inspect(test_dtm)
inspect(test_dtm)
train_labels <- style[train_index]
test_labels <- style[-train_index]
classifier <- naiveBayes(as.matrix(train_dtm), train_labels)
predict(classifier, "Secondo me Salvini rappresenta la sola speranza per questo paese")
predict(classifier, "Secondo me Salvini rappresenterebbe un pessimo premier orrendo")
predict(classifier,
"Secondo me rappresenterebbe un pessimo premier orrendo")
warnings()
pred <- predict(classifier, test_corpus)
pred <- predict(classifier, test_dtm)
pred <- predict(classifier, as.matrix(test_dtm))
warnings()
#creazione di una dtm
corpus <- VCorpus(VectorSource(testi_lemmatizzati))
dtm <- DocumentTermMatrix(corpus)
dtm_train <- dtm[train_index]
dtm_test <- dtm[test_index]
dtm_test <- dtm[-train_index]
test_index <- !sentiment_values_dataframe$index %in% train_index
test_index
train_index
test_index <- sentiment_values_dataframes$index[
!sentiment_values_dataframe$index %in% train_index]
test_index <- sentiment_values_dataframe$index[
!sentiment_values_dataframe$index %in% train_index]
length(train_index)
test_index
length(test_index)
dtm_test <- dtm[test_index]
dtm_train <- dtm[train_index]
dtm_test <- dtm[test_index]
inspect(dtm_test)
dt_test
dtm_test
dtm
inspect(dtm)
length(dtm)
nrow(dtm)
as.matrix(dtm)
as.matrix(dtm)[train_index]
dtm_train <- as.matrix(dtm)[train_index]
dtm_test <- as.matrix(dtm)[test_index]
sum(dtm_test[1])
sum(dtm_test[2])
dtm_test[1]
dtm_test
dtm_train <- as.matrix(dtm)[train_index]
dtm
dtm$dimnames$Docs
dtm_train <- dtm[dtm$dimnames$Docs %in% train_index]
dtm_train <- dtm[dtm$dimnames$Docs %in% train_index,]
dtm_test <- dtm[dtm$dimnames$Docs %in% test_index,]
train_labels <- style[train_index]
test_labels <- style[-train_index]
inspect(dtm_test)
classifier <- naiveBayes(
as.matrix(train_dtm),
train_labels)
predict(classifier,
"Secondo me Salvini rappresenta la sola speranza per questo paese")
frase_prova <- test_corpus[1]
frase_prova
frase_prova <- testi_lemmatizzati[test_index][1]
frase_prova
frase_prova <- testi[test_index][1]
frase_prova
testi[test_index][1]
frase_prova <- testi_lemmatizzati[test_index][1]
frase_prova
test_index[1]
predict(classifier,
as.matrix(test_dtm[2]))
predict(classifier,
as.matrix(dtm[2]))
dtm[1]
predict(classifier,
as.matrix(dtm)[2])
warnings()
pred <- predict(classifier, as.matrix(test_dtm))
warnings()
test_dfm <- dfm_select(test_dfm, train_dfm)
library(quanteda)
test_dfm <- dfm_select(test_dfm, train_dfm)
test_dfm <- dfm_select(dfm_test, dfm_train)
test_dfm <- dfm_select(dtm_test, dtm_train)
classifier <- naiveBayes(
as.matrix(train_dtm),
train_labels)
hist(pred)
table(pred)
pred <- predict(classifier, as.matrix(dtm_test))
table(pred)
dtm_train
#creazione di una dtm
corpus <- VCorpus(VectorSource(testi_lemmatizzati))
dtm <- DocumentTermMatrix(corpus)
dtm
#creazione di una dtm
corpus <- VCorpus(VectorSource(paste(testi_lemmatizzati))
#creazione di una dtm
corpus <- VCorpus(VectorSource(paste(testi_lemmatizzati)))
paste(testi_lemmatizzati)
length(paste(testi_lemmatizzati))
length(paste(testi_lemmatizzati, sep=' '))
paste(testi_lemmatizzati, sep=' ')[0]
paste(testi_lemmatizzati, sep=' ')[1]
testi_lemmatizzati[1]
paste(testi_lemmatizzati[[1]])
paste(testi_lemmatizzati[1])
VectorSource(testi_lemmatizzati[1])
VCorpus(VectorSource(testi_lemmatizzati[1]))
DocumentTermMatrix(VCorpus(VectorSource(testi_lemmatizzati[1])))
class(testi_lemmatizzati)
class(c(testi_lemmatizzati))
class(as.character(testi_lemmatizzati))
head(as.character(testi_lemmatizzati))
head(as.character(testi_lemmatizzati))[1]
testi_lemmatizzati[1]
for (i in unique(data$doc_id)) {
tokens <- data[data$doc_id==i,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
testi_lemmatizzati <- c(testi_lemmatizzati, tokens)
}
testi_lemmatizzati <- c()
for (i in unique(data$doc_id)) {
tokens <- data[data$doc_id==i,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
testi_lemmatizzati <- c(testi_lemmatizzati, tokens)
}
testi_lemmatizzati[1]
for (i in unique(data$doc_id)) {
tokens <- data[data$doc_id==i,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
testi_lemmatizzati <- c(testi_lemmatizzati, paste(tokens))
}
testi_lemmatizzati <- c()
for (i in unique(data$doc_id)) {
tokens <- data[data$doc_id==i,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
testi_lemmatizzati <- c(testi_lemmatizzati, paste(tokens))
}
testi_lemmatizzati[1]
tokens
paste(tokens)
list(paste(tokens))
class(list(paste(tokens)))
class((paste(tokens))
class(paste(tokens))
class(paste(tokens))
token_from_lemmas <- function(id){
tokens <- data[data$doc_id==id,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
return(paste(tokens))
}
doc_ids <- unique(data$doc_id)
doc_ids
#ricarichiamo il nostro file annotato sintatticamente
data <- read_csv('./materiali/annotazioni-sintattiche.csv')
doc_ids <- unique(data$doc_id)
doc_ids
token_from_lemmas <- function(id){
tokens <- data[data$doc_id==id,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
return(paste(tokens))
}
testi_lemmatizzati <- lapply(data, token_from_lemmas)
testi_lemmatizzati[1]
token_from_lemmas <- function(id){
tokens <- data[data$doc_id==id,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
return(tokens)
}
testi_lemmatizzati <- lapply(data, token_from_lemmas)
class(testi_lemmatizzati)
testi_lemmatizzati[1]
return(list(tokens))
token_from_lemmas <- function(id){
tokens <- data[data$doc_id==id,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
return(list(tokens))
}
testi_lemmatizzati <- lapply(data, token_from_lemmas)
testi_lemmatizzati[1]
testi_lemmatizzati[2]
testi_lemmatizzati <- lapply(data, token_from_lemmas)
testi_lemmatizzati[2]
length(testi_lemmatizzati)
testi_lemmatizzati <- lapply(doc_ids, token_from_lemmas)
length(testi_lemmatizzati)
class(testi_lemmatizzati)
class(testi_lemmatizzati[1])
testi_lemmatizzati[1]
return(paste(tokens))
token_from_lemmas <- function(id){
tokens <- data[data$doc_id==id,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
return(paste(tokens))
}
testi_lemmatizzati <- lapply(doc_ids, token_from_lemmas)
testi_lemmatizzati[1]
class(testi_lemmatizzati)
return(as.character(tokens))
token_from_lemmas <- function(id){
tokens <- data[data$doc_id==id,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
return(as.character(tokens))
}
testi_lemmatizzati <- lapply(doc_ids, token_from_lemmas)
class(testi_lemmatizzati)
testi_lemmatizzati[1]
paste(tokens)
paste(tokens[1])
paste(tokens)
class(tokens)
paste(tokens)
paste(unlist(tokens))
paste(unlist(tokens[1]))
paste(unlist(tokens))
paste((tokens))
paste((tokens))[1]
paste((tokens))
class(paste((tokens)))
return(paste(tokens))
token_from_lemmas <- function(id){
tokens <- data[data$doc_id==id,]$lemma
tokens <- unlist(tolower(tokens[!tokens %in% sw]))
return(paste(tokens))
}
testi_lemmatizzati <- lapply(doc_ids, token_from_lemmas)
testi_lemmatizzati[1]
class(tokens)
tokens[1]
tokens[2]
tokens[3]
testi_lemmatizzati[1]
testi_lemmatizzati[2]
data_annotato$Style[data_annotato$Style == "positivr"] <- "positive"
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
data_annotato$Style)
style <- data_annotato$Style
table(style)
sentiment_values_dataframe$polarity <- ifelse(
sentiment_values_dataframe$sentiment_values >= 0.7,
"positivo",
ifelse(sentiment_values_dataframe$sentiment_values <= 0.5,
"negativo",
"neutro")
)
table(sentiment_values_dataframe$polarity)
table(style)
#creazione di un indice per il training
train_index <- sample(nrow(sentiment_values_dataframe), 3000)
test_index <- sentiment_values_dataframe$index[
!sentiment_values_dataframe$index %in% train_index]
length(train_index)
#creazione di una dtm
corpus <- VCorpus(VectorSource(paste(testi_lemmatizzati)))
dtm <- DocumentTermMatrix(corpus)
dtm
inspect(dtm)
length(paste(testi_lemmatizzati, sep=' '))
DocumentTermMatrix(VCorpus(VectorSource(testi_lemmatizzati[1])))
DocumentTermMatrix(VCorpus(VectorSource(testi_lemmatizzati[2])))
DocumentTermMatrix(VCorpus(VectorSource(testi_lemmatizzati[1:10])))
DocumentTermMatrix(VCorpus(VectorSource(testi_lemmatizzati[1:])))
DocumentTermMatrix(VCorpus(VectorSource(testi_lemmatizzati[1:10000])))
dtm_train <- dtm[dtm$dimnames$Docs %in% train_index,]
dtm_test <- dtm[dtm$dimnames$Docs %in% test_index,]
train_labels <- style[train_index]
test_labels <- style[-train_index]
classifier <- naiveBayes(
as.matrix(train_dtm),
train_labels)
table(train_labels)
pred <- predict(classifier, as.matrix(dtm_test))
table(pred)
classifier
confusionMatrix(test_labels, pred)
confusionMatrix(as.factor(test_labels[,1]), pred)
as.factor(test_labels)
confusionMatrix(as.factor(test_labels), pred)
as.factor(test_labels)
confusionMatrix(as.factor(test_labels), pred)
confusionMatrix(pred, as.factor(test_labels))
table(style)
as.factor(test_labels)
as.factor(test_labels)
test_labels
pred
pos_ids <- data_annotato$id[data_annotato$Style == 'positive']
pos_texts <- VCorpus(VectorSource(lemmatized_texts[pos_ids]))
pos_texts <- VCorpus(VectorSource(testi_lemmatizzati[pos_ids]))
dtm_test
inspect(dtm_test)
dtm_train
dtm_train <- removeSparseTerms(dtm_train, 0.5)
dtm_train
dtm_train <- removeSparseTerms(dtm_train, 0.5)
dtm_test <- removeSparseTerms(dtm_test, 0.5)
train_labels <- style[train_index]
test_labels <- style[-train_index]
classifier <- naiveBayes(
as.matrix(dtm_train),
train_labels)
classifier
pred <- predict(classifier, as.matrix(dtm_test))
confusionMatrix(pred, as.factor(test_labels))
table(pred)
pos_ids <- data_annotato$id[data_annotato$Style == 'positive']
pos_texts <- VCorpus(VectorSource(testi_lemmatizzati[pos_ids]))
wordcloud(pos_texts, max.words=50, scale=c(4,0.5))
pos_ids <- data_annotato$id[data_annotato$Style == 'positive']
pos_texts <- VCorpus(VectorSource(testi_lemmatizzati[pos_ids]))
wordcloud(pos_texts, max.words=50, scale=c(4,0.5))
pos_ids <- data_annotato$id[data_annotato$Style == 'positive']
pos_ids
head(data_annotato)
colnames(data_annotato)
data_annotato$id <- seq(from=1, to=nrow(data_annotato), by=1)
pos_ids <- data_annotato$id[data_annotato$Style == 'positive']
pos_texts <- VCorpus(VectorSource(testi_lemmatizzati[pos_ids]))
wordcloud(pos_texts, max.words=50, scale=c(4,0.5))
neg_texts <- VCorpus(VectorSource(lemmatized_texts[neg_ids]))
wordcloud(neg_texts, max.words=50, scale=c(4,0.5))
neg_texts <- VCorpus(VectorSource(testi_lemmatizzati[neg_ids]))
neg_ids <- data_annotato$id[data_annotato$Style == 'negative']
neg_texts <- VCorpus(VectorSource(testi_lemmatizzati[neg_ids]))
wordcloud(neg_texts, max.words=50, scale=c(4,0.5))
wd_pos <- wordcloud(pos_texts, max.words=50, scale=c(4,0.5))
wd_neg <- wordcloud(neg_texts, max.words=50, scale=c(4,0.5))
par(mfrow=c(wd_neg, wd_pos))
wd_neg
wordcloud(pos_texts, max.words=50, scale=c(4,0.5))
wordcloud(neg_texts, max.words=50, scale=c(4,0.5))
confusionMatrix(pred, as.factor(test_labels))
length(pred)
length(test_labels)
levels(test_labels)
levels(as.factor(test_labels))
data_annotato$Style[data_annotato$Style == "emphasising candidates' values"] <- "not applicable"
test_labels <- style[-train_index]
confusionMatrix(pred, as.factor(test_labels))
as.factor(test_labels)
style <- data_annotato$Style
confusionMatrix(pred, as.factor(test_labels))
as.factor(test_labels)
test_labels <- style[-train_index]
as.factor(test_labels)
data_annotato$Style[data_annotato$Style == "positivr"] <- "positive"
data_annotato$Style[data_annotato$Style == "emphasising candidates' values"] <- "not applicable"
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
data_annotato$Style)
style <- data_annotato$Style
table(style)
data_annotato$Style[data_annotato$Style == "emphasising candidate's values"] <- "not applicable"
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
data_annotato$Style)
style <- data_annotato$Style
table(style)
test_labels <- style[-train_index]
as.factor(test_labels)
confusionMatrix(pred, as.factor(test_labels))
table(pred)
confusionMatrix(pred, as.factor(test_labels))
