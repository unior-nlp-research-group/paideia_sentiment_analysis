library(tidyverse)
library(tokenizers)
library(udpipe)
library(stopwords)
library(R.utils)
library(tm)
library(parallel)
library(readxl)
library(data.table)
# importiamo il dataset
my_data <- read_excel("../materiali/dataset.xlsx")
my_data <- data.frame(my_data)
library(readxl)
# importiamo il dataset
my_data <- read_excel("../materiali/dataset.xlsx")
library(tidyverse)
library(ggplot2)
library(tokenizers)
library(stopwords)
library(tidytext)
library(tm)
library(irr)
library(readxl)
#ricarichiamo il nostro file di testo
my_data <- read_excel("./materiali/dataset.xlsx")
warnings()
testi <- my_data$Text
#tokenizzazione
tokens_full <- tokenize_words(testi)
tokens_full <- unlist(tokens_full)
class(tokens_full)
head(tokens_full)
tokens_full
tokens_full[[1]]
tokens_full[1]
testi <- my_data$Text
#tokenizzazione
tokens_full <- tokenize_words(testi)
class(tokens_full)
head(tokens_full)
head(tokens_full)
tokens_full  <- unlist(tokens_full)
class(tokens_full)
head(tokens_full)
tokens_full  <- unlist(tokens_full)
head(tokens_full)
# ottenere la frequenza dei token
tables(tokens_full)
# ottenere la frequenza dei token
table(tokens_full)
freq_parole <- as.data.frame(table(tokens_full))
# esaminare una parola di interesse
parola_di_interesse <- "politiche"
tokens==parola_di_interesse
tokens_full==parola_di_interesse
num_occorrenze <- freq_parole[freq_parole$tokens_full == parola_di_interesse,]
num_occorrenze$Freq
ud_it <- udpipe_load_model(file='./materiali/italian-isdt-ud-2.5-191206.udpipe')
library(udpipe)
ud_it <- udpipe_load_model(file='./materiali/italian-isdt-ud-2.5-191206.udpipe')
udpipe.annotate(ud_it,
x="Questo è il corso di sentiment analysis per le scienze sociali",
doc_id = 1)
udpipe_annotate(ud_it,
x="Questo è il corso di sentiment analysis per le scienze sociali",
doc_id = 1)
annotazione = as.data.table(udpipe_annotate(ud_it,
x="Questo è il corso di sentiment analysis per le scienze sociali",
doc_id = 1))
library(data.table)
annotazione = as.data.table(udpipe_annotate(ud_it,
x="Questo è il corso di sentiment analysis per le scienze sociali",
doc_id = 1))
annotazione
annotazione
annotazione[annotazione$upos=='PRON']
annotazione
annotazione[annotazione$lemma == 'il']
annotazione[annotazione$lemma == 'il']$doc_id
unique(annotazione[annotazione$lemma == 'il']$doc_id)
library(tidyverse)
library(tokenizers)
library(udpipe)
library(stopwords)
library(R.utils)
library(tm)
library(parallel)
library(readxl)
library(data.table)
# importiamo il dataset
my_data <- read_excel("../materiali/dataset.xlsx")
my_data <- data.frame(my_data)
class(my_data)
head(my_data)
colnames(my_data)
nrow(my_data)
testi <- my_data$Text
#tokenizzazione
tokens <- tokenize_words(testi)
length(tokens)
tokens <- unlist(tokens)
length(tokens)
#tokens <- unlist(tokenize_words(testo))
cat("Numero totale di token:", length(tokens))
#calcolo di ttr
types <- unique(tokens)
cat("Numero totale di token unici:", length(types))
ttr <- length(types) / length(tokens)
cat("Valore di ttr:", ttr)
testi[2]
str_extract(testi[2], regex("raccontar[a-z]*"))
pattern = regex("#.*")
str_extract(testi, pattern)
pattern = regex("#[A-Za-z0-9àèéòìù]*")
str_extract(testi, pattern)
clean_text <- function(text){
text <- tolower(text)
text <- gsub(".", " ", text, fixed=TRUE)
text <- gsub(":", " ", text, fixed=TRUE)
text <- gsub("?", " ", text, fixed=TRUE)
text <- gsub("!", " ", text, fixed=TRUE)
text <- gsub("; ", " ", text, fixed=TRUE)
text <- gsub(", ", " ", text, fixed=TRUE)
text <- gsub("\ `", " ", text, fixed=TRUE)
text <- gsub("\n", " ", text, fixed=TRUE)
text <- gsub("\r", " ", text, fixed=TRUE)
return(text)
}
my_data$clean_text <- clean_text(my_data$Text)
# analisi sintattica
ud_it <- udpipe_load_model(file='./materiali/italian-isdt-ud-2.5-191206.udpipe')
annotazione = as.data.table(udpipe_annotate(ud_it,
x="Questo è il corso di sentiment analysis per le scienze sociali",
doc_id = 1))
# scriviamo una funzione per estrarre info sintattiche
annotate_splits <- function(x, file) {
ud_model <- udpipe_load_model(file)
x <- as.data.table(udpipe_annotate(ud_model,
x = x$clean_text,
doc_id = x$id))
return(x)
}
# load parallel library future.apply
library(future.apply)
# numero di core da utilizzare
ncores <- 3L
plan(multiprocess, workers = ncores)
#prima inseriamo una colonna per identificare gli id dei testi nel dataframe
my_data$id <- seq(1:nrow(my_data))
# dividere il corpus
corpus_splitted <- split(my_data, seq(1, nrow(my_data), by = 1000))
annotation <- future_lapply(corpus_splitted, annotate_splits, file = '../materiali/italian-isdt-ud-2.5-191206.udpipe')
# analisi sintattica
ud_it <- udpipe_load_model(file='../materiali/italian-isdt-ud-2.5-191206.udpipe')
# analisi sintattica
ud_it <- udpipe_load_model(file='./materiali/italian-isdt-ud-2.5-191206.udpipe')
annotation <- future_lapply(corpus_splitted, annotate_splits, file = './materiali/italian-isdt-ud-2.5-191206.udpipe')
annotation <- rbindlist(annotation)
head(annotation)
write.csv(annotation, 'annotazioni-sintattiche.csv')
#ricarichiamo il nostro file annotato sintatticamente
data <- read_csv('./annotazioni-sintattiche.csv')
library(tidyverse)
library(ggplot2)
library(tokenizers)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(udpipe)
library(stopwords)
library(R.utils)
library(tm)
library(readxl)
library(RTextTools)
library(e1071)
library(caret)
#ricarichiamo il nostro file annotato sintatticamente
data <- read_csv('./annotazioni-sintattiche.csv')
class(data)
data <- as.data.frame(data)
lemmatized_texts <- c()
for (i in unique(data$doc_id)) {
tokens <- data[data$doc_id==i,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
#text <- paste(tokens, collapse=' ')
lemmatized_texts <- c(lemmatized_texts, list(tokens))
}
lemmatized_texts[1]
sw <- stopwords("it")
lemmatized_texts <- c()
for (i in unique(data$doc_id)) {
tokens <- data[data$doc_id==i,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
#text <- paste(tokens, collapse=' ')
lemmatized_texts <- c(lemmatized_texts, list(tokens))
}
lemmatized_texts[1]
data_annotato <- read_excel("../materiali/dataset.xlsx")
data_annotato <- data.frame(my_data)
testi <- data_annotato$Text
style <- data_annotato$Style
data_annotato$Style[my_data$Style == "positivr"] = "positive"
data_annotato$Style <- ifelse(is.na(my_data$Style),
"not applicable",
my_data$Style)
#importare lessico esterno
nrc_table <- as.data.frame(
fread('../materiali/NRC-VAD-Lexicon-Aug2018Release/OneFilePerLanguage/Italian-it-NRC-VAD-Lexicon.txt'
)
)
#importare lessico esterno
nrc_table <- as.data.frame(
fread('./materiali/NRC-VAD-Lexicon-Aug2018Release/OneFilePerLanguage/Italian-it-NRC-VAD-Lexicon.txt'
)
)
calcolare_sentiment_nrc <- function(frase_input){
tokens <- unlist(tokenize_words(frase_input))
tokens_clean <- tokens[!tokens %in% sw]
sentiment_values <- nrc_table[is.element(nrc_table$`Italian-it`, tokens_clean),]$Valence
return(mean(sentiment_values))
}
sentiment_values <- unlist(lapply(lemmatized_texts, calcolare_sentiment_nrc))
sentiment_values[is.na(sentiment_values)] <- 0
sentiment_values_dataframe <- data.frame(
testi, sentiment_values
)
sentiment_values_dataframe  <- sentiment_values_dataframe[
order(-sentiment_values_dataframe$sentiment_values)
,]
sentiment_values_dataframe$index <- seq(
from=1,
to=nrow(sentiment_values_dataframe),
by=1)
colnames(sentiment_values_dataframe)
head(sentiment_values_dataframe)
sentiment_values_dataframe$polarity <- ifelse(
sentiment_values_dataframe$sentiment_values >= 0.7,
"positivo",
ifelse(sentiment_values_dataframe$sentiment_values <= 0.5,
"negativo",
"neutro")
)
table(sentiment_values_dataframe$polarity)
table(style)
lemmatized_texts
# addestramento di un classificatore naive bayes
#creazione corpus
lemmatized_texts
corpus <- VCorpus(VectorSource((lemmatized_texts)))
corpus[1]
corpus
content(corpus[1])
content(corpus[[1]])
# creare matrice document-termini
dtm <- DocumentTermMatrix(corpus, control = list(
tolower = FALSE,
removeNumbers = TRUE
))
dtm
dtm[[1]]
dtm[[2]]
dtm[[3]]
length(lemmatized_texts)
tm::inspect(dtm)
train_split <- dtm[1:3000]
test_split <- dtm[30001:]
test_split <- dtm[3001:]
test_split <- dtm[3001:3792]
labels_train <- style[1:3000]
labels_test <- style[3000:3792]
prop.table(labels_train)
prop.table(table(labels_train))
"
"
data_annotato$Style[my_data$Style == "positivr"] = "positive"
data_annotato$Style <- ifelse(is.na(my_data$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
labels_train <- style[1:3000]
labels_test <- style[3000:3792]
prop.table(table(labels_train))
data_annotato$Style[my_data$Style == "positivr"] = "positive"
data_annotato$Style <- ifelse(is.na(my_data$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato$Style[my_data$Style == "positivr"] <- "positive"
data_annotato$Style <- ifelse(is.na(my_data$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato[my_data$Style == "positivr"]$Style <- "positive"
data_annotato$Style <- ifelse(is.na(my_data$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
data_annotato[my_data$Style == "positivr",]$Style <- "positive"
data_annotato$Style <- ifelse(is.na(my_data$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato$Style[my_data$Style == "positivr"] <- "positive"
style <- data_annotato$Style
table(style)
data_annotato$Style <- ifelse(is.na(my_data$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato$Style[my_data$Style == "positivr"] <- "positive"
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato$Style[data_annotato$Style == "positivr"] <- "positive"
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
my_data$Style)
style <- data_annotato$Style
table(style)
data_annotato$Style[data_annotato$Style == "positivr"] <- "positive"
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
data_annotato$Style)
style <- data_annotato$Style
table(style)
labels_train <- style[1:3000]
labels_test <- style[3000:3792]
prop.table(table(labels_train))
train_dtm
train_split
train_index = sample(3792, 3000)
data_annotato
head(data_annotato)
train_sentences <- lemmatized_texts[train_index]
test_sentences <- lemmatized_texts[-train_index]
train_corpus <- VCorpus(VectorSource(train_sentences))
test_corpus <- VCorpus(VectorSource(test_sentences))
train_dtm <- DocumentTermMatrix(train_corpus)
test_dtm <- DocumentTermMatrix(test_corpus)
tm::inspect(test_dtm)
train_labels <- style[train_index]
test_labels <- style[-train_index]
head(train_labels)
classifier <- naiveBayes(as.matrix(train_dtm), train_labels)
predict(classifier, "Secondo me Salvini rappresenta la sola speranza per questo paese")
predict(classifier, "Secondo me Salvini rappresenterebbe un pessimo premier")
predict(classifier, "Secondo me salvino rappresenterebbe un pessimo premier")
predict(classifier, "Secondo me salvino rappresenterebbe un pessimo premier orrendo")
wordcloud(train_sentences, max.words=40, scale=c(3,0.5))
wordcloud(lemmatized_texts[train_index], max.words=40, scale=c(3,0.5))
class(train_sentences)
wordcloud(train_corpus, max.words=40, scale=c(3,0.5))
pos <- subset(data_annotato, Style == 'positive')
wordcloud(pos$Text, max.words=40, scale=c(3,0.5))
wordcloud(pos$Text, max.words=40, scale=c(3,10))
wordcloud(pos$Text, max.words=40, scale=c(3,0.1))
wordcloud(pos$Text, max.words=50, scale=c(3,0.1))
pos_ids <- data_annotato$id[data_annotato$Style == 'positve']
pos_ids <- data_annotato$id[data_annotato$Style == 'positive']
wordcloud(lemmatized_texts[pos_ids], max.words=50, scale=c(3,0.1))
lemmatied_texts[pos_ids]
lemmatized_texts[pos_ids]
pos_texts <- lemmatized_texts[pos_ids]
wordcloud(pos_texts, max.words=50, scale=c(3,0.1))
pos_texts <- VCorpus(VectorSource(lemmatized_texts[pos_ids]))
wordcloud(pos_texts, max.words=50, scale=c(3,0.1))
wordcloud(pos_texts, max.words=50, scale=c(2,0.1))
wordcloud(pos_texts, max.words=50, scale=c(3,0.1))
wordcloud(pos_texts, max.words=50, scale=c(3,0.5))
wordcloud(pos_texts, max.words=50, scale=c(4,0.5))
neg_ids <- data_annotato$id[data_annotato$Style == 'negative']
neg_texts <- VCorpus(VectorSource(lemmatized_texts[neg_ids]))
wordcloud(neg_texts, max.words=50, scale=c(4,0.5))
wordcloud(pos_texts, max.words=50, scale=c(4,0.5))
wordcloud(neg_texts, max.words=50, scale=c(4,0.5))
predict(classifier, "Secondo me Salvini rappresenta la sola speranza per questo paese")
predict(classifier, "Secondo me Salvini rappresenterebbe un pessimo premier orrendo")
librerie <- c('tidyverse',
'tokenizers',
'udpipe',
'tm',
'stopwords',
'R.utils',
"tidytext",
'textdata',
"readxl",
"parallel",
"future.apply",
"irr",
"RTextTools",
"e1071",
"rlang",
"hardhat",
"recipes",
"caret"
)
library(tidyverse)
library(tokenizers)
library(udpipe)
library(stopwords)
library(R.utils)
library(tm)
library(parallel)
library(readxl)
library(data.table)
# importiamo il dataset
my_data <- read_excel("./materiali/dataset.xlsx")
my_data <- data.frame(my_data)
clean_text <- function(text){
text <- tolower(text)
text <- gsub(".", " ", text, fixed=TRUE)
text <- gsub(":", " ", text, fixed=TRUE)
text <- gsub("?", " ", text, fixed=TRUE)
text <- gsub("!", " ", text, fixed=TRUE)
text <- gsub("; ", " ", text, fixed=TRUE)
text <- gsub(", ", " ", text, fixed=TRUE)
text <- gsub("\ `", " ", text, fixed=TRUE)
text <- gsub("\n", " ", text, fixed=TRUE)
text <- gsub("\r", " ", text, fixed=TRUE)
return(text)
}
my_data$clean_text <- clean_text(my_data$Text)
# analisi sintattica
ud_it <- udpipe_load_model(file='./materiali/italian-isdt-ud-2.5-191206.udpipe')
# scriviamo una funzione per estrarre info sintattiche
annotate_splits <- function(x) {
ud_model <- ud_it
x <- as.data.table(udpipe_annotate(ud_model,
x = x$clean_text,
doc_id = x$id))
return(x)
}
# load parallel library future.apply
library(future.apply)
# numero di core da utilizzare
ncores <- 3L
plan(multiprocess, workers = ncores)
#prima inseriamo una colonna per identificare gli id dei testi nel dataframe
my_data$id <- seq(1:nrow(my_data))
# dividere il corpus
corpus_splitted <- split(my_data, seq(1, nrow(my_data), by = 1000))
annotation <- future_lapply(corpus_splitted, annotate_splits)
annotation <- rbindlist(annotation)
head(annotation)
write.csv(annotation, 'annotazioni-sintattiche.csv')
