hist(x=x[[1]])
ggplot(x[[1]]) +
geom_bar()
df1 <- data.frame(seq(1:5), x[[1]])
df1 <- data.frame("rating" = seq(1:5), "values" = x[[1]])
ggplot(df1) +
geom_bar()
ggplot(df1) +
geom_bar(x = values)
ggplot(df1) +
geom_bar(x = df1$values)
ggplot(df1) +
geom_bar(aes(x=values))
ggplot(df1) +
geom_bar(aes(x=values, y=rating), stat = unique())
geom_bar(aes(x=values, y=rating), stat = "identity"
ggplot(df1) +
geom_bar(aes(x=values, y=rating), stat = "identity")
ggplot(df1) +
geom_bar(aes(x=values, y=rating), stat ="identity")
ggplot(df1) +
geom_bar(aes(x=ratng, y=values), stat ="identity")
ggplot(df1) +
geom_bar(aes(x=rating, y=values), stat ="identity")
df2 <- data.frame("rating" = seq(1:5), "values" = x[[2]])
ggplot(df1) +
geom_bar(aes(x=rating, y=values), stat ="identity")
lista_parole <- c("bellissimo", "brutto", "batteria")
x <- lapply(lista_parole, find_occ)
df2 <- data.frame("rating" = seq(1:5), "values" = x[[2]])
ggplot(df1) +
geom_bar(aes(x=rating, y=values), stat ="identity")
total_freqs[total_freqs$tokens == "brutto"]
"brutto" %in% tokens_full
ggplot(df2) +
geom_bar(aes(x=rating, y=values), stat ="identity")
df3 <- data.frame("rating" = seq(1:5), "values" = x[[3]])
ggplot(df3) +
geom_bar(aes(x=rating, y=values), stat ="identity")
getFreq <- function(rating){
recensioni <- my_data$V3[my_data$V1 == rating]
tokens <- unlist(tokenize_words(recensioni))
tokens <- factor(tokens, levels = unique(tokens_full))
table_tokens <- table(tokens)
df <- data.frame(table_tokens)
df <- df[order(-df$Freq),]
return(df)
}
ratings <- seq(1:5)
freqs <-lapply(ratings, getFreq)
total_freqs <- rbindlist(freqs, use.names = TRUE, idcol="rating")
total_freqs[total_freqs$tokens == 'ottimo' & total_freqs$rating == 5]
find_occ <- function(parola){
if (parola %in% tokens_full){
occ <- total_freqs[
total_freqs$tokens == parola]$Freq
return(occ)
}
else
{
return(c(0,0,0,0,0))
}
}
find_occ <- function(parola){
if (parola %in% tokens_full){
occ <- total_freqs[
total_freqs$tokens == parola]$Freq
return(occ)
}
else
{
return(c(0,0,0,0,0))
}
}
lista_parole <- c("bellissimo", "brutto", "batteria")
x <- lapply(lista_parole, find_occ)
x
df3 <- data.frame("rating" = seq(1:5), "values" = x[[3]])
ggplot(df3) +
geom_bar(aes(x=rating, y=values), stat ="identity")
annotazioni.sintattiche <- read.csv("~/Progetti/paideia_sentiment_analysis/materiali/annotazioni-sintattiche.csv")
View(annotazioni.sintattiche)
annotazioni.sintattiche <- read.csv("~/Progetti/paideia_sentiment_analysis/materiali/annotazioni-sintattiche.csv")
View(annotazioni.sintattiche)
#ricarichiamo il nostro file annotato sintatticamente
data <- read.csv("~/Progetti/paideia_sentiment_analysis/materiali/annotazioni-sintattiche.csv")
#ricarichiamo il nostro file annotato sintatticamente
data <- read.csv("./materiali/annotazioni-sintattiche.csv")
class(data)
colnames(data)
dataset <- read_excel("materiali/dataset.xlsx")
data_annotato <- data.frame(data_annotato)
data_annotato <- data.frame(dataset)
doc_ids <- unique(data$doc_id)
head(data)
colnames(data)
head(unique(data$doc_id))
head(unique(data$paragraph_id))
head(unique(data$sentence_id))
head(unique(data$doc_id))
data$doc_id
doc_ids <- unique(data$doc_id)
token_from_lemmas <- function(id){
tokens <- data[data$doc_id==id,]$lemma
tokens <- unlist(tolower(tokens[!tokens %in% sw]))
return(paste(tokens))
}
testi_lemmatizzati <- lapply(doc_ids, token_from_lemmas)
testi_lemmatizzati[1]
testi_lemmatizzati[2]
testi_lemmatizzati[10]
length(testi_lemmatizzati)
data_annotato$Style[data_annotato$Style == "positivr"] <- "positive"
length(testi_lemmatizzato)
length(testi_lemmatizzati)
data_annotato$Style[data_annotato$Style == "positivr"] <- "positive"
data_annotato$Style[data_annotato$Style == "emphasising candidate's values"] <- "not applicable"
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
data_annotato$Style)
style <- data_annotato$Style
data_annotato$id <- seq(from=1, to=nrow(data_annotato), by=1)
table(style)
#importare lessico esterno
lessico <- as.data.frame(
fread('./materiali/sentix'
)
)
head(lessico)
parola <- "mendace"
lessico[lessico$V1 == parola,]
lessico[lessico$V1 == parola,]$V4
lessico[lessico$V1 == parola,]
lessico[lessico$V1 == parola,]$V4
mean(lessico[lessico$V1 == parola,]$V4)
parola <- "razzista"
mean(lessico[lessico$V1 == parola,]$V4)
parola <- "razzismo"
mean(lessico[lessico$V1 == parola,]$V4)
parola <- "razza"
mean(lessico[lessico$V1 == parola,]$V4)
parola <- "finto"
mean(lessico[lessico$V1 == parola,]$V4)
parola <- "sufficiente"
mean(lessico[lessico$V1 == parola,]$V4)
parola <- "deficiente"
parola <- "deficente"
mean(lessico[lessico$V1 == parola,]$V4)
parola <- "deficiente"
mean(lessico[lessico$V1 == parola,]$V4)
parola <- "cattolico"
mean(lessico[lessico$V1 == parola,]$V4)
lessico[lessico$V1 == parola,]
parola <- "sinistra"
lessico[lessico$V1 == parola,]
parola <- "destra"
lessico[lessico$V1 == parola,]
parola <- "movimento"
lessico[lessico$V1 == parola,]
parola <- "ottimismo"
lessico[lessico$V1 == parola,]
parola <- "fiducia"
lessico[lessico$V1 == parola,]
mean(lessico[lessico$V1 == parola,]$V4)
parola <- "amore"
mean(lessico[lessico$V1 == parola,]$V4)
parola <- "tasse"
mean(lessico[lessico$V1 == parola,]$V4)
parola <- "tassa"
mean(lessico[lessico$V1 == parola,]$V4)
parola <- "non"
mean(lessico[lessico$V1 == parola,]$V4)
frase <- c("Sono molto fiducioso per il futuro, anche se mi sento un po' preoccupato a causa degli ultimi avvenimenti")
tokens_frase <- tokenize_words(frase)
tokens_frase <- unlist(tokens_frase)
tokens_frase <- tokens_frase[!tokens_frase %in% sw]
is.element(lessivo$V1, tokens_frase)
is.element(lessico$V1, tokens_frase)
lessico[is.element(lessico$V1, tokens_frase),]
frase
tokens_frase
tokens_frase
head(lessico)
is.element(lessico$V1, tokens_frase)
lessico[is.element(lessico$V1, tokens_frase),]
lessico[is.element(lessico$V1, tokens_frase),]$V4
sentiment_values <- lessico[is.element(lessico$V1, tokens_frase),]$V4
sentiment_values
mean(sentiment_values)
round(mean(sentiment_values))
round(mean(sentiment_values),2)
round(mean(sentiment_values),3)
calcolare_sentiment <- function(frase_input){
tokens <- unlist(tokenize_words(frase_input))
tokens_clean <- tokens[!tokens %in% sw]
sentiment_values <- lessico[is.element(lessico$V1, tokens_clean),]$V4
return(mean(sentiment_values))
}
sentiment_values <- unlist(
lapply(
testi_lemmatizzati, calcolare_sentiment))
hist(sentiment_values)
calcolare_sentiment <- function(frase_input){
tokens <- unlist(tokenize_words(frase_input))
tokens_clean <- tokens[!tokens %in% sw]
sentiment_values <- lessico[is.element(lessico$V1, tokens_clean),]$V6
return(mean(sentiment_values))
}
sentiment_values <- unlist(
lapply(
testi_lemmatizzati, calcolare_sentiment))
hist(sentiment_values)
density(sentiment_values)
sentiment_values[is.na(sentiment_values)] <- 0
sentiment_values_dataframe <- data.frame(
testi, sentiment_values
)
density(sentiment_values)
density(sentiment_values)
hist(density(sentiment_values))
density(sentiment_values)
frase <- c("Sono molto fiducioso per il futuro, anche se mi sento un po' preoccupato a causa degli ultimi avvenimenti")
tokens_frase <- tokenize_words(frase)
tokens_frase <- unlist(tokens_frase)
tokens_frase <- tokens_frase[!tokens_frase %in% sw]
lessico[is.element(lessico$V1, tokens_frase),]$V4
hist(lessico[is.element(lessico$V1, tokens_frase),]$V4)
lessico[is.element(lessico$V1, tokens_frase),]$V4)
lessico[is.element(lessico$V1, tokens_frase),]$V4
polarity <- lessico[is.element(lessico$V1, tokens_frase),]$V4
tokens_interest <- lessico[is.element(lessico$V1, tokens_frase),]$V1
df <- data.frame(
polarity,
"tokens" = tokens_interest
)
df
ggplot(df)+
geom_bar(aes(x=tokens, y = polarity), stat="identity")
df
head(df)
ggplot(df)+
geom_bar(aes(x=tokens, y = polarity), stat="mean")
ggplot(df)+
geom_bar(aes(x=tokens, y = polarity), stat="avg")
ggplot(df)+
geom_bar(aes(x=tokens, y = polarity), stat="mean")
ggplot(df)+
geom_bar(aes(x=tokens, y = polarity), stat="identity")
ggplot(df)+
geom_bar(aes(x=tokens, y = polarity), stat="summary", fun="mean")
sentiment_values[is.na(sentiment_values)] <- 0
hist(sentiment_values)
calcolare_sentiment <- function(frase_input){
tokens <- unlist(tokenize_words(frase_input))
tokens_clean <- tokens[!tokens %in% sw]
sentiment_values <- lessico[is.element(lessico$V1, tokens_clean),]$V4
return(mean(sentiment_values))
}
sentiment_values <- unlist(
lapply(
testi_lemmatizzati, calcolare_sentiment))
sentiment_values[is.na(sentiment_values)] <- 0
hist(sentiment_values)
sentiment_values_dataframe <- data.frame(
testi, sentiment_values
)
testi <- data_annotato$Text
sentiment_values_dataframe <- data.frame(
testi, sentiment_values
)
head(sentiment_values_dataframe)
sentiment_values_dataframe  <- sentiment_values_dataframe[
order(-sentiment_values_dataframe$sentiment_values)
,]
head(sentiment_values_dataframe)
sentiment_values_dataframe$index <- seq(
from=1,
to=nrow(sentiment_values_dataframe),
by=1)
sentiment_values_dataframe$polarity <- ifelse(
sentiment_values_dataframe$sentiment_values > 0.4,
"positivo",
ifelse(sentiment_values_dataframe$sentiment_values < 0.2,
"negativo",
"neutro")
)
head(sentiment_values_dataframe$polarity)
table(sentiment_values_dataframe$polarity)
sentiment_values_dataframe$polarity <- ifelse(
sentiment_values_dataframe$sentiment_values > 0.4,
"positivo",
ifelse(sentiment_values_dataframe$sentiment_values < 0.2,
"negativo",
"neutro")
)
table(sentiment_values_dataframe$polarity)
table(sentiment_values_dataframe$polarity)
# creare split
set.seed(12)
#creazione di un indice per il training
sample(10, 100)
#creazione di un indice per il training
sample(100, 10)
#creazione di un indice per il training
length(sentiment_values_dataframe)
sample(1, 100)
sample(100, 1)
sample(1, 100, replace=TRUE)
#creazione di un indice per il training
nrow(sentiment_values_dataframe)
#creazione di un indice per il training
train_index <- sample(nrow(sentiment_values_dataframe), 3000)
train_index
test_index <- sentiment_values_dataframe$index[
!sentiment_values_dataframe$index %in% train_index]
length(train_index)
length(test_index)
#creazione di una dtm
corpus <- VCorpus(VectorSource(paste(testi_lemmatizzati)))
dtm <- DocumentTermMatrix(corpus)
inspect(dtm)
tm::inspect(dtm)
dtm$dimnames$Docs
dtm_train <- dtm[dtm$dimnames$Docs %in% train_index,]
dtm_test <- dtm[dtm$dimnames$Docs %in% test_index,]
dtm_train
train_labels <- style[train_index]
test_labels <- style[-train_index]
table(train_labels)
classifier <- naiveBayes(
as.matrix(dtm_train),
train_labels)
library(irr)
classifier <- naiveBayes(
as.matrix(dtm_train),
train_labels)
library(e1071)
classifier <- naiveBayes(
as.matrix(dtm_train),
train_labels)
pred <- predict(classifier, as.matrix(dtm_test))
library(tidyverse)
library(ggplot2)
library(tokenizers)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(udpipe)
library(stopwords)
library(R.utils)
library(data.table)
library(tm)
library(readxl)
library(RTextTools)
library(e1071)
library(caret)
#importiamo le librerie che useremo in questa giornata
library(tidyverse)
library(ggplot2)
library(tokenizers)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(udpipe)
library(stopwords)
library(R.utils)
library(data.table)
library(tm)
library(readxl)
library(RTextTools)
library(e1071)
library(caret)
sw <- stopwords("it")
#ricarichiamo il nostro file annotato sintatticamente
data <- read.csv("./materiali/annotazioni-sintattiche.csv")
class(data)
colnames(data)
#data <- as.data.frame(data)
dataset <- read_excel("materiali/dataset.xlsx")
data_annotato <- data.frame(dataset)
head(data)
data$doc_id
doc_ids <- unique(data$doc_id)
token_from_lemmas <- function(id){
tokens <- data[data$doc_id==id,]$lemma
tokens <- unlist(tolower(tokens[!tokens %in% sw]))
return(paste(tokens))
}
testi_lemmatizzati <- lapply(doc_ids, token_from_lemmas)
testi_lemmatizzati[2]
length(testi_lemmatizzati)
data_annotato$Style[data_annotato$Style == "positivr"] <- "positive"
data_annotato$Style[data_annotato$Style == "emphasising candidate's values"] <- "not applicable"
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
data_annotato$Style)
style <- data_annotato$Style
data_annotato$id <- seq(from=1, to=nrow(data_annotato), by=1)
table(style)
# utilizzo dei lessici per la sentiment
#link nrc
# http://saifmohammad.com/WebPages/AccessResource.htm
#link sentix
# http://valeriobasile.github.io/twita/downloads.html
#importare lessico esterno
lessico <- as.data.frame(
fread('./materiali/sentix'
)
)
head(lessico)
#**andiamo a vedere i valori di una parola esemplificativa**
parola <- "non"
mean(lessico[lessico$V1 == parola,]$V4)
#calcoliamo il valore per una frase
frase <- c("Sono molto fiducioso per il futuro, anche se mi sento un po' preoccupato a causa degli ultimi avvenimenti")
tokens_frase <- tokenize_words(frase)
tokens_frase <- unlist(tokens_frase)
tokens_frase <- tokens_frase[!tokens_frase %in% sw]
polarity <- lessico[is.element(lessico$V1, tokens_frase),]$V4
tokens_interest <- lessico[is.element(lessico$V1, tokens_frase),]$V1
df <- data.frame(
polarity,
"tokens" = tokens_interest
)
head(df)
ggplot(df)+
geom_bar(aes(x=tokens, y = polarity), stat="summary", fun="mean")
sentiment_values <- lessico[is.element(lessico$V1, tokens_frase),]$V4
sentiment_values
round(mean(sentiment_values),3)
#**scriviamo una funzione per calcolare il valore di valenza data una frase**
calcolare_sentiment <- function(frase_input){
tokens <- unlist(tokenize_words(frase_input))
tokens_clean <- tokens[!tokens %in% sw]
sentiment_values <- lessico[is.element(lessico$V1, tokens_clean),]$V4
return(mean(sentiment_values))
}
sentiment_values <- unlist(
lapply(
testi_lemmatizzati, calcolare_sentiment))
hist(sentiment_values)
#quanti NA ci sono?
sentiment_values[is.na(sentiment_values)] <- 0
density(sentiment_values)
testi <- data_annotato$Text
sentiment_values_dataframe <- data.frame(
testi, sentiment_values
)
head(sentiment_values_dataframe)
sentiment_values_dataframe  <- sentiment_values_dataframe[
order(-sentiment_values_dataframe$sentiment_values)
,]
head(sentiment_values_dataframe)
sentiment_values_dataframe$index <- seq(
from=1,
to=nrow(sentiment_values_dataframe),
by=1)
colnames(sentiment_values_dataframe)
head(sentiment_values_dataframe)
# impostiamo dei limiti (threshold):
# negativo  <0.2
# neutro  >=0.4
# positivo  > 0.4
sentiment_values_dataframe$polarity <- ifelse(
sentiment_values_dataframe$sentiment_values > 0.4,
"positivo",
ifelse(sentiment_values_dataframe$sentiment_values < 0.2,
"negativo",
"neutro")
)
head(sentiment_values_dataframe$polarity)
table(sentiment_values_dataframe$polarity)
table(style)
# addestramento di un classificatore naive bayes
# creare split
set.seed(12)
#creazione di un indice per il training
train_index <- sample(nrow(sentiment_values_dataframe), 3000)
train_index
test_index <- sentiment_values_dataframe$index[
!sentiment_values_dataframe$index %in% train_index]
length(test_index)
#creazione di una dtm
corpus <- VCorpus(VectorSource(paste(testi_lemmatizzati)))
dtm <- DocumentTermMatrix(corpus)
dtm$dimnames$Docs
dtm_train <- dtm[dtm$dimnames$Docs %in% train_index,]
dtm_test <- dtm[dtm$dimnames$Docs %in% test_index,]
dtm_train
train_labels <- style[train_index]
test_labels <- style[-train_index]
table(train_labels)
library(e1071)
classifier <- naiveBayes(
as.matrix(dtm_train),
train_labels)
pred <- predict(classifier, as.matrix(dtm_test))
confusionMatrix(pred, as.factor(test_labels))
dtm_train <- removeSparseTerms(dtm_train, 0.5)
dtm_test <- removeSparseTerms(dtm_test, 0.5)
classifier <- naiveBayes(
as.matrix(dtm_train),
train_labels)
pred <- predict(classifier, as.matrix(dtm_test))
confusionMatrix(pred, as.factor(test_labels))
pos_ids <- data_annotato$id[data_annotato$Style == 'positive']
pos_texts <- VCorpus(VectorSource(testi_lemmatizzati[pos_ids]))
wordcloud(pos_texts, max.words=50, scale=c(4,0.5))
wordcloud(neg_texts, max.words=50, scale=c(4,0.5))
neg_ids <- data_annotato$id[data_annotato$Style == 'negative']
neg_texts <- VCorpus(VectorSource(testi_lemmatizzati[neg_ids]))
wordcloud(neg_texts, max.words=50, scale=c(4,0.5))
