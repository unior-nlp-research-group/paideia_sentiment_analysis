wordcloud(pos_texts, max.words=50, scale=c(3,0.5))
wordcloud(pos_texts, max.words=50, scale=c(4,0.5))
neg_ids <- data_annotato$id[data_annotato$Style == 'negative']
neg_texts <- VCorpus(VectorSource(lemmatized_texts[neg_ids]))
wordcloud(neg_texts, max.words=50, scale=c(4,0.5))
wordcloud(pos_texts, max.words=50, scale=c(4,0.5))
wordcloud(neg_texts, max.words=50, scale=c(4,0.5))
predict(classifier, "Secondo me Salvini rappresenta la sola speranza per questo paese")
predict(classifier, "Secondo me Salvini rappresenterebbe un pessimo premier orrendo")
librerie <- c('tidyverse',
'tokenizers',
'udpipe',
'tm',
'stopwords',
'R.utils',
"tidytext",
'textdata',
"readxl",
"parallel",
"future.apply",
"irr",
"RTextTools",
"e1071",
"rlang",
"hardhat",
"recipes",
"caret"
)
library(tidyverse)
library(tokenizers)
library(udpipe)
library(stopwords)
library(R.utils)
library(tm)
library(parallel)
library(readxl)
library(data.table)
# importiamo il dataset
my_data <- read_excel("./materiali/dataset.xlsx")
my_data <- data.frame(my_data)
clean_text <- function(text){
text <- tolower(text)
text <- gsub(".", " ", text, fixed=TRUE)
text <- gsub(":", " ", text, fixed=TRUE)
text <- gsub("?", " ", text, fixed=TRUE)
text <- gsub("!", " ", text, fixed=TRUE)
text <- gsub("; ", " ", text, fixed=TRUE)
text <- gsub(", ", " ", text, fixed=TRUE)
text <- gsub("\ `", " ", text, fixed=TRUE)
text <- gsub("\n", " ", text, fixed=TRUE)
text <- gsub("\r", " ", text, fixed=TRUE)
return(text)
}
my_data$clean_text <- clean_text(my_data$Text)
# analisi sintattica
ud_it <- udpipe_load_model(file='./materiali/italian-isdt-ud-2.5-191206.udpipe')
# scriviamo una funzione per estrarre info sintattiche
annotate_splits <- function(x) {
ud_model <- ud_it
x <- as.data.table(udpipe_annotate(ud_model,
x = x$clean_text,
doc_id = x$id))
return(x)
}
# load parallel library future.apply
library(future.apply)
# numero di core da utilizzare
ncores <- 3L
plan(multiprocess, workers = ncores)
#prima inseriamo una colonna per identificare gli id dei testi nel dataframe
my_data$id <- seq(1:nrow(my_data))
# dividere il corpus
corpus_splitted <- split(my_data, seq(1, nrow(my_data), by = 1000))
annotation <- future_lapply(corpus_splitted, annotate_splits)
annotation <- rbindlist(annotation)
head(annotation)
write.csv(annotation, 'annotazioni-sintattiche.csv')
install.packages(jsonlite)
library(jsonlite)
install.packages("jsonlite")
install.packages("jsonlite")
library(jsonlite)
files <- list.files(path='./materiali/annotazioni_amazon/',
pattern="utente_paideia*",
full.names=TRUE)
files
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
library(dplyr)
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
class(dfs[[1]])
dfs <- lapply(files, readfiles)
files <- list.files(path='./materiali/annotazioni_amazon',
pattern="utente_paideia*",
full.names=TRUE)
files
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
files <- list.files(path='./materiali/annotazioni_amazon',
full.names=TRUE)
files
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
files <- list.files(path='./materiali/annotazioni_amazon',
pattern='utente_paideia*'
full.names=TRUE)
files
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
class(dfs[[1]])
files <- list.files(path='./materiali/annotazioni_amazon',
pattern='utente_paideia*',
full.names=TRUE)
files
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
class(dfs[[1]])
colnames(dfs[[1]])
length(dfs)
dataframe_id1_annotatore1 <- dfs[[1]]$entities.id1
dataframe_id1_annotatore1
dataframe_id1_annotatore1 <- dfs[[1]]$entities.id1
dataframe_id2_annotatore1 <- dfs[[2]]$entities.id1
dataframe_id3_annotatore1 <- dfs[[3]]$entities.id1
dataframe_id4_annotatore1 <- dfs[[4]]$entities.id1
dataframe_id5_annotatore1 <- dfs[[5]]$entities.id1
dataframe_id6_annotatore1 <- dfs[[6]]$entities.id1
dataframe_id7_annotatore1 <- dfs[[7]]$entities.id1
dataframe_id8_annotatore1 <- dfs[[8]]$entities.id1
dataframe_id9_annotatore1 <- dfs[[9]]$entities.id1
dataframe_id10_annotatore1 <- dfs[[10]]$entities.id1
dataframe_id11_annotatore1 <- dfs[[11]]$entities.id1
dataframe_id12_annotatore1 <- dfs[[12]]$entities.id1
dataframe_id13_annotatore1 <- dfs[[13]]$entities.id1
kappam.fleiss(c(
dataframe_id1_annotatore1,
dataframe_id1_annotatore1
dataframe_id1_annotatore1
))
dataframe_id1_annotatore1# analisi IAA
kappam.fleiss(c(
dataframe_id1_annotatore1,
dataframe_id2_annotatore1,
dataframe_id3_annotatore1,
dataframe_id4_annotatore1,
dataframe_id5_annotatore1,dataframe_id6_annotatore1,
dataframe_id8_annotatore1,dataframe_id7_annotatore1,
dataframe_id9_annotatore1,dataframe_id10_annotatore1,
dataframe_id11_annotatore1, dataframe_id12_annotatore1
dataframe_id1_annotatore1,dataframe_id1_annotatore1
))
kappam.fleiss(c(
dataframe_id1_annotatore1,
dataframe_id2_annotatore1,
dataframe_id3_annotatore1,
dataframe_id4_annotatore1,
dataframe_id5_annotatore1,dataframe_id6_annotatore1,
dataframe_id8_annotatore1,dataframe_id7_annotatore1,
dataframe_id9_annotatore1,dataframe_id10_annotatore1,
dataframe_id11_annotatore1,dataframe_id12_annotatore1)
))
kappam.fleiss(c(
dataframe_id1_annotatore1,
dataframe_id2_annotatore1,
dataframe_id3_annotatore1,
dataframe_id4_annotatore1,
dataframe_id5_annotatore1,dataframe_id6_annotatore1,
dataframe_id8_annotatore1,dataframe_id7_annotatore1,
dataframe_id9_annotatore1,dataframe_id10_annotatore1,
dataframe_id11_annotatore1,dataframe_id12_annotatore1))
install.packages(kappam)
install.packages("kappam")
library(kappam)
library(irr)
kappam.fleiss(c(
dataframe_id1_annotatore1,
dataframe_id2_annotatore1,
dataframe_id3_annotatore1,
dataframe_id4_annotatore1,
dataframe_id5_annotatore1,dataframe_id6_annotatore1,
dataframe_id8_annotatore1,dataframe_id7_annotatore1,
dataframe_id9_annotatore1,dataframe_id10_annotatore1,
dataframe_id11_annotatore1,dataframe_id12_annotatore1))
dataframe_id1_annotatore1 <- dfs[[1]]$entities.id1[dfs[[1]]$entities.id1==NA] <- 'None'
dataframe_id1_annotatore1 <- dfs[[1]]$entities.id1[dfs[[1]]$entities.id1==NA] <- 'None'
dataframe_id2_annotatore1 <- dfs[[2]]$entities.id1[dfs[[2]]$entities.id1==NA] <- 'None'
dataframe_id3_annotatore1 <- dfs[[3]]$entities.id1[dfs[[3]]$entities.id1==NA] <- 'None'
dataframe_id4_annotatore1 <- dfs[[4]]$entities.id1[dfs[[4]]$entities.id1==NA] <- 'None'
dataframe_id5_annotatore1 <- dfs[[5]]$entities.id1[dfs[[5]]$entities.id1==NA] <- 'None'
dataframe_id6_annotatore1 <- dfs[[6]]$entities.id1[dfs[[6]]$entities.id1==NA] <- 'None'
dataframe_id6_annotatore1
dataframe_id7_annotatore1 <- dfs[[7]]$entities.id1[dfs[[7]]$entities.id1==NA] <- 'None'
dataframe_id7_annotatore1
dataframe_id1_annotatore1
dfs[[1]]$entities.id1==NA
is.na(dfs[[1]]$entities.id1)
dataframe_id1_annotatore1 <- dfs[[1]]$entities.id1[is.na(dfs[[1]]$entities.id1)] <- 'None'
dataframe_id1_annotatore1
dfs[[1]]$entities.id1
files <- list.files(path='./materiali/annotazioni_amazon',
pattern='utente_paideia*',
full.names=TRUE)
files
readfiles <- function(filename){
lines <- readLines(filename)
lines <- lapply(lines, fromJSON)
lines <- lapply(lines, unlist)
df <- bind_rows(lines)
}
dfs <- lapply(files, readfiles)
dataframe_id6_annotatore1 <- dfs[[6]]$entities.id1
dataframe_id1_annotatore1 <- dfs[[1]]$entities.id1
dataframe_id2_annotatore1 <- dfs[[2]]$entities.id1
dataframe_id4_annotatore1 <- dfs[[4]]$entities.id1
dataframe_id5_annotatore1 <- dfs[[5]]$entities.id1
dataframe_id7_annotatore1 <- dfs[[7]]$entities.id1
dataframe_id1_annotatore1[is.na(dataframe_id1_annotatore1)] <- "None"
dataframe_id1_annotatore1
dataframe_id1_annotatore1[is.na(dataframe_id1_annotatore1)] <- "None"
dataframe_id2_annotatore1[is.na(dataframe_id2_annotatore1)] <- "None"
dataframe_id4_annotatore1[is.na(dataframe_id4_annotatore1)] <- "None"
dataframe_id5_annotatore1[is.na(dataframe_id5_annotatore1)] <- "None"
dataframe_id7_annotatore1[is.na(dataframe_id7_annotatore1)] <- "None"
kappam.fleiss(c(
dataframe_id1_annotatore1,
dataframe_id2_annotatore1,
dataframe_id4_annotatore1,
dataframe_id5_annotatore1,
dataframe_id7_annotatore1))
dataframe_id1_annotatore1
#importiamo le librerie che useremo in questa giornata
library(tidyverse)
library(ggplot2)
library(tokenizers)
library(stopwords)
library(tidytext)
library(udpipe)
library(tm)
library(irr)
library(readxl)
library(data.table)
Reviews_Dooge_X10 <- read.csv("~/Progetti/paideia_sentiment_analysis/materiali/Reviews_Dooge_X10.csv", header=FALSE)
View(Reviews_Dooge_X10)
#ricarichiamo il nostro dataset
my_data <- read.csv("~/Progetti/paideia_sentiment_analysis/materiali/Reviews_Dooge_X10.csv", header=FALSE)
my_data
my_data <- data.frame(my_data)
class(my_data)
colnames(my_data)
#ricarichiamo il nostro dataset
my_data <- read.csv("~/Progetti/paideia_sentiment_analysis/materiali/Reviews_Dooge_X10.csv",
header=FALSE)
my_data <- data.frame(my_data)
colnames(my_data)
head(my_data$V1)
head(my_data$V2)
head(my_data$V3)
tokens <- tokenize_words(my_data$V3)
tokens[1]
tokens <- unlist(tokens)
head(tokens)
summary(tokens)
table(tokens)
tokens_df <- data.frame(table(tokens))
head(tokens_df)
head(tokens)
table(tokens)
library(tidyverse)
library(ggplot2)
library(tokenizers)
library(stopwords)
library(tidytext)
library(udpipe)
library(tm)
library(irr)
library(readxl)
library(data.table)
head(tokens_df)
length(tokens)
head(tokens_df)
sw <- stopwords("it")
sw
stopwords("en")
sw <- stopwords("it")
sw
tokens[tokens!=sw]
tokens %in% sw
tokens != "None"
tokens <- tokens[!tokens %in% sw]
head(tokens)
tokens <- tokens[!tokens %in% sw]
tokens_df <- data.frame(table(tokens))
head(tokens_df)
tokens_df[order(tokens_df$Freq)]
tokens_df[order(tokens_df$Freq),]
colnames(tokens_df)
tokens_df$Freq
order(tokens_df$Freq)
head(tokens_df[order(tokens_df$Freq),],10)
head(tokens_df[!order(tokens_df$Freq),],10)
head(tokens_df[order(-tokens_df$Freq),],10)
nrow(my_data)
## 1. dividiamo il dataset per rating
colnames(my_data)
head(my_data$1)
head(my_data$V1)
recensioni_5 <- my_data$V3[my_data$V1==5]
head(recensioni_5)
my_data$V1[1]
sum(my_data$V1==5)
table(my_data$V1)
recensioni_5 <- my_data$V3[my_data$V1==5]
recensioni_4 <- my_data$V3[my_data$V1==4]
recensioni_3 <- my_data$V3[my_data$V1==3]
recensioni_2 <- my_data$V3[my_data$V1==2]
recensioni_1 <- my_data$V3[my_data$V1==1]
class(recensione_5)
class(recensioni_5)
tokens <- tokenize_words(recensioni_5)
tokens[1]
tokens_5 <- unlist(tokenize_words(recensioni_5))
tokens_4 <- unlist(tokenize_words(recensioni_4))
tokens_3 <- unlist(tokenize_words(recensioni_3))
tokens_2 <- unlist(tokenize_words(recensioni_2))
tokens_1 <- unlist(tokenize_words(recensioni_1))
freq_5 <- data.frame(table(tokens_5))
freq_4 <- data.frame(table(tokens_4))
freq_3 <- data.frame(table(tokens_3))
freq_2 <- data.frame(table(tokens_2))
freq_1 <- data.frame(table(tokens_1))
head(freq_4)
freq_5 <- freq_5[order(-freq_5$Freq)]
freq_5 <- freq_5[order(-freq_5$Freq),]
freq_5 <- freq_5[order(-freq_5$Freq),]
freq_4 <- freq_5[order(-freq_4$Freq),]
freq_3 <- freq_5[order(-freq_3$Freq),]
freq_2 <- freq_5[order(-freq_2$Freq),]
freq_1 <- freq_5[order(-freq_1$Freq),]
freq_5 <- freq_5[order(-freq_5$Freq)]
head(freq_1)
freq_5 <- freq_5[order(-freq_5$Freq),]
freq_4 <- freq_4[order(-freq_4$Freq),]
freq_3 <- freq_3[order(-freq_3$Freq),]
freq_2 <- freq_2[order(-freq_2$Freq),]
freq_1 <- freq_1[order(-freq_1$Freq),]
head(freq_1)
colnames(freq_5)
freq_5 <- freq_5[!freq_5$tokens_5 %in% sw]
freq_5 <- freq_5[!freq_5$tokens_5 %in% sw,]
freq_5
head(freq_5)
freq_1 <- freq_1[!freq_1$tokens_1 %in% sw,]
head(freq_1)
freq_1 <- freq_1[!freq_1$tokens_1 %in% sw,]
head(freq_1)
head(freq_1)
freq_1 <- freq_1[order(-freq_1$Freq),]
head(freq_1)
head(freq_2)
colnames(freq_1)
freq_1 <- data.frame(table(tokens_1))
colnames(freq_1)
freq_5 <- data.frame(table(tokens_5))
freq_4 <- data.frame(table(tokens_4))
freq_3 <- data.frame(table(tokens_3))
freq_2 <- data.frame(table(tokens_2))
freq_1 <- data.frame(table(tokens_1))
freq_5 <- freq_5[order(-freq_5$Freq),]
freq_4 <- freq_4[order(-freq_4$Freq),]
freq_3 <- freq_3[order(-freq_3$Freq),]
freq_2 <- freq_2[order(-freq_2$Freq),]
freq_1 <- freq_1[order(-freq_1$Freq),]
colnames(freq_1)
freq_1 <- freq_1[!freq_1$tokens_1 %in% sw,]
head(freq_1)
"dopo" %in% sw
head(freq_3)
freq_3 <- freq_3[!freq_3$tokens_3 %in% sw,]
freq_3
recensioni_3 <- my_data$V3[my_data$V1==3]
tokens_3 <- unlist(tokenize_words(recensioni_3))
freq_3 <- data.frame(table(tokens_3))
freq_3 <- freq_3[order(-freq_3$Freq),]
freq_3 <- freq_3[!freq_3$tokens_3 %in% sw,]
freq_3
head(freq_3)
library(tidyverse)
library(ggplot2)
library(tokenizers)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(udpipe)
library(stopwords)
library(R.utils)
library(tm)
library(readxl)
library(RTextTools)
library(e1071)
library(caret)
sw <- stopwords("it")
#ricarichiamo il nostro file annotato sintatticamente
data <- read_csv('./annotazioni-sintattiche.csv')
class(data)
#ricarichiamo il nostro file annotato sintatticamente
data <- read_csv('../materiali/annotazioni-sintattiche.csv')
#ricarichiamo il nostro file annotato sintatticamente
data <- read_csv('./materiali/annotazioni-sintattiche.csv')
class(data)
data <- as.data.frame(data)
data
colnames(data)
lemmatized_texts <- c()
for (i in unique(data$doc_id)) {
tokens <- data[data$doc_id==i,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
#text <- paste(tokens, collapse=' ')
lemmatized_texts <- c(lemmatized_texts, list(tokens))
}
lemmatized_texts[1]
data_annotato <- read_excel("./materiali/dataset.xlsx")
data_annotato <- data.frame(my_data)
data_annotato <- data.frame(data_annotato)
testi <- data_annotato$Text
for (i in unique(data$doc_id)) {
tokens <- data[data$doc_id==i,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
testi_lemmatizzati <- c(testi_lemmatizzati, list(tokens))
}
testi_lemmatizzati <- c()
for (i in unique(data$doc_id)) {
tokens <- data[data$doc_id==i,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
testi_lemmatizzati <- c(testi_lemmatizzati, list(tokens))
}
testi_lemmatizzati[1]
data_annotato <- read_excel("./materiali/dataset.xlsx")
data_annotato <- data.frame(data_annotato)
testi <- data_annotato$Text
data_annotato$Style[data_annotato$Style == "positivr"] <- "positive"
data_annotato$Style <- ifelse(is.na(data_annotato$Style),
"not applicable",
data_annotato$Style)
style <- data_annotato$Style
#importare lessico esterno
nrc_table <- as.data.frame(
fread('./materiali/sentix'
)
)
library(data.table)
#importare lessico esterno
nrc_table <- as.data.frame(
fread('./materiali/sentix'
)
)
#importare lessico esterno
lessico <- as.data.frame(
fread('./materiali/sentix'
)
)
head(lessico)
frase <- c("Sono molto fiducioso per il futuro, anche se mi sento un po' preoccupato a causa degli ultimi avvenimenti")
tokens_frase <- tokenize_words(frase)
tokens_frase <- unlist(tokens_frase)
tokens_frase <- tokens_frase[!tokens_frase %in% sw]
nrc_table[is.element(lessico$V1, tokens_frase),]
lessico[is.element(lessico$V1, tokens_frase),]
sentiment_values <- lessico[is.element(lessico$V1, tokens_frase),]$V6
sentiment_values
lessico[is.element(lessico$V1, tokens_frase),]
sentiment_values <- lessico[is.element(lessico$V1, tokens_frase),]$V6
sentiment_values
calcolare_sentiment <- function(frase_input){
tokens <- unlist(tokenize_words(frase_input))
tokens_clean <- tokens[!tokens %in% sw]
sentiment_values <- lessico[is.element(lessico$V1, tokens_clean),]$V6
return(mean(sentiment_values))
}
sentiment_values <- unlist(lapply(lemmatized_texts, calcolare_sentiment))
head(sentiment_values)
is.na(sentiment_values)
