neg_texts[1]
neg_texts[2]
neg_texts[3]
neg_texts[4]
pos_texts <- lemmatized_texts[data_annotato$Style=='positive']
# la funzione successiva serve a calcolare le probabilità di una sequenza di token
calc_probs <- function(tokens){
counts <- table(tokens) + 1
return(log(counts/sum(counts)))
}
pos_probs <- calc_probs(pos_texts)
# la funzione successiva serve a calcolare le probabilità di una sequenza di token
calc_probs <- function(tokens){
tokens <- unlist(tokens)
counts <- table(tokens) + 1
return(log(counts/sum(counts)))
}
pos_probs <- calc_probs(pos_texts)
pos_probs
neg_probs <- calc_probs(neg_texts)
# la funzione successiva serve a calcolare le probabilità di una sequenza di token
all_tokens <- unlist(lemmatized_texts)
all_tokens <- unlist(lemmatized_texts)
neg_texts <- lemmatized_texts[data_annotato$Style=='negative']
pos_texts <- lemmatized_texts[data_annotato$Style=='positive']
calc_probs <- function(tokens){
tokens <- unlist(tokens)
counts <- table(tokens) + 1
return(log(counts/sum(counts)))
}
counts <- table(tokens, layer(all_tokens)) + 1
calc_probs <- function(tokens){
tokens <- unlist(tokens)
counts <- table(tokens, layer(all_tokens)) + 1
return(log(counts/sum(counts)))
}
pos_probs <- calc_probs(pos_texts)
neg_probs <- calc_probs(neg_texts)
pos_probs
all_tokens
pos_pred
calc_sentiment <- function(review){
test <- tokenize(review)
pos_pred <- sum(
is.na(pos_probs[test])
)
pos_pred
}
calc_sentiment("Salvini è un ottimo premier")
calc_sentiment <- function(review){
test <- tokenize(review)
print(test)
pos_pred <- sum(
is.na(pos_probs[test])
)
pos_pred
}
calc_sentiment("Salvini è un ottimo premier")
calc_sentiment <- function(frase){
test <- tokenize(review)
print(test)
pos_pred <- sum(
is.na(pos_probs[test])
)
pos_pred
}
calc_sentiment("Salvini è un ottimo premier")
calc_sentiment <- function(frase){
test <- tokenize(frase)
print(test)
pos_pred <- sum(
is.na(pos_probs[test])
)
pos_pred
}
calc_sentiment("Salvini è un ottimo premier")
calc_sentiment("Salvini è un ottimo premier")
calc_sentiment <- function(frase){
print(frase)
pos_pred <- sum(
is.na(pos_probs[test])
)
pos_pred
}
calc_sentiment("Salvini è un ottimo premier")
calc_sentiment <- function(frase){
test <- tokenize_word(frase)
print(test)
pos_pred <- sum(
is.na(pos_probs[test])
)
pos_pred
}
calc_sentiment("Salvini è un ottimo premier")
library(tidyverse)
library(ggplot2)
library(tokenizers)
library(wordcloud)
library(RColorBrewer)
library(SnowballC)
library(udpipe)
library(stopwords)
library(R.utils)
library(tm)
library(readxl)
library(RTextTools)
library(e1071)
library(caret)
calc_sentiment <- function(frase){
test <- tokenize_words(frase)
print(test)
pos_pred <- sum(
is.na(pos_probs[test])
)
pos_pred
}
calc_sentiment("Salvini è un ottimo premier")
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test])
)
pos_pred
}
calc_sentiment("Salvini è un ottimo premier")
is.na(pos_probs['salvini'])
is.na(pos_probs['Salvini'])
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test])+sum(pos_probs[test])
)
}
calc_sentiment("Salvini è un ottimo premier")
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test])+sum(pos_probs[test])
)
print(pos_pred)
}
calc_sentiment("Salvini è un ottimo premier")
sum(pos_probs['ottimo'])
sum(pos_probs['premier'])
pos_pred
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test])+sum(pos_probs[test])
)
pos_pred
}
calc_sentiment("Salvini è un ottimo premier")
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test]))+sum(pos_probs[test]
)
pos_pred
}
calc_sentiment("Salvini è un ottimo premier")
t <- unlist(tokenize_words("Salvini sarebbe un pessimo premier"))
is.na(pos_probs[t])
sum(is.na(pos_probs[t]))
pos_probs_rare <- calc_probs_rare(pos_texts)
calc_probs_rare <- function(tokens){
tokens <- unlist(tokens)
counts <- table(tokens) + 1
return(log(1/sum(counts)))
}
pos_probs_rare <- calc_probs_rare(pos_texts)
neg_probs_rare <- calc_probs_rare(neg_texts)
t <- unlist(tokenize_words("Salvini sarebbe un pessimo premier"))
sum(is.na(pos_probs[t]))
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test])*pos_probs_rare)+sum(pos_probs[test]
)
pos_pred
}
calc_sentiment("Salvini è un ottimo premier")
sum(is.na(pos_probs[t]))*pos_probs_rare
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test]))*pos_probs_rare+sum(pos_probs[test]
)
pos_pred
}
calc_sentiment("Salvini è un ottimo premier")
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test]))*pos_probs_rare+sum(pos_probs[test], na.rm=TRUE)
pos_pred
}
calc_sentiment("Salvini è un ottimo premier")
calc_sentiment("Salvini è un pesismo premier")
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test]))*pos_probs_rare+sum(pos_probs[test], na.rm=TRUE)
neg_pred <- sum(
is.na(neg_probs[test]))*neg_probs_rare+sum(neg_probs[test], na.rm=TRUE)
ifelse(pos_pred>neg_pred, "positivo", "negativo")
}
calc_sentiment("Salvini è un pesismo premier")
calc_sentiment("Salvini è un ottimo premier")
calc_sentiment("Salvini è un bellissimo fantastico premier che renderà l'Italia estremamente ricca")
calc_sentiment("bellissimo fantastico premier che renderà l'Italia estremamente ricca")
calc_sentiment("molto bello")
calc_sentiment("sono molto felice")
frase <- "sono molto felice"
t<-tokenize_words(frase)
t<-unlist(tokenize_words(frase)
t<-unlist(tokenize_words(frase))
t <- unlist(tokenize_words(frase))
pos_probs[t]
neg_probs[t]
pos_texts
pos_texts[1]
unlist(pos_texts)
pos_probs <- calc_probs(unlist(pos_texts))
neg_probs <- calc_probs(unlist(neg_texts))
pos_probs
calc_probs <- function(tokens){
tokens <- unlist(tokens)
counts <- table(tokens) + 1
return(log(counts/sum(counts)))
}
pos_probs <- calc_probs(unlist(pos_texts))
neg_probs <- calc_probs(unlist(neg_texts))
pos_probs
table(pos_probs)
table(pos_texts)
table(unlist(pos_texts))
table(unlist(pos_texts))/sum(table(unlist(tokens)))
calc_sentiment("2018")
frase <- "2018"
t <- unlist(tokenize_words(frase))
neg_probs[t]
pos_probs[t]
pos_texts
calc_probs <- function(tokens){
counts <- table(tokens) + 1
log(counts/sum(counts))
}
pos_probs <- calc_probs(unlist(pos_texts))
neg_probs <- calc_probs(unlist(neg_texts))
pos_probs
table(unlist(pos_texts))/sum(table(unlist(tokens)))
table(unlist(pos_texts))+1/sum(table(unlist(tokens)))
pos_probs[7]]
pos_probs[7]
pos_probs["7"]
neg_texts <- lemmatized_texts[data_annotato$Style=='negative']
pos_texts <- lemmatized_texts[data_annotato$Style=='positive']
pos_texts
neg_texts <- unlist(lemmatized_texts[data_annotato$Style=='negative'])
pos_texts <- unlist(lemmatized_texts[data_annotato$Style=='positive'])
pos_texts
table(pos_texts)
table(pos_texts)/sum(table(pos_texts))
log(table(pos_texts)/sum(table(pos_texts)))
table(pos_texts)
table(pos_texts["Salvini"])
pos_probs["Salvini"]
neg_probs["Salvini"]
neg_probs["perfetto"]
pos_probs["perfetto"]
sum(pos_probs)
sum(neg_probs)
calc_probs <- function(tokens){
counts <- table(tokens) + 1
counts/sum(counts)
}
pos_probs <- calc_probs(unlist(pos_texts))
sum(pos_probs)
pos_probs["Salvini"]
class(pos_probs)
pos_probs[order]
pos_probs
ps
ps
ps <- as.data.frame(pos_probs)
ps
colnames(ps)
ps[order(ps$Freq)]
ps[order(ps$Freq),]
ps[order(-ps$Freq),]
head(ps[order(-ps$Freq),])
neg_probs <- calc_probs(unlist(neg_texts))
table(unlist(pos_texts))+1/sum(table(unlist(tokens)))
calc_probs_rare <- function(tokens){
tokens <- unlist(tokens)
counts <- table(tokens) + 1
return(log(1/sum(counts)))
}
pos_probs_rare <- calc_probs_rare(pos_texts)
neg_probs_rare <- calc_probs_rare(neg_texts)
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test]))*pos_probs_rare+sum(pos_probs[test], na.rm=TRUE)
neg_pred <- sum(
is.na(neg_probs[test]))*neg_probs_rare+sum(neg_probs[test], na.rm=TRUE)
ifelse(pos_pred>neg_pred, "positivo", "negativo")
}
frase <- "essere"
t <- unlist(tokenize_words(frase))
pos_probs[t]
calc_sentiment("esser")
frase <- "essere"
t <- unlist(tokenize_words(frase))
neg_probs[t]
sum(neg_texts=="essere")
neg_texts
sum(pos_texts=="essere")
neg_texts <- unlist(lemmatized_texts[data_annotato$Style=='negative'])
pos_texts <- unlist(lemmatized_texts[data_annotato$Style=='positive'])
sum(pos_texts=="essere")
sum(pos_texts=="Salvini")
sum(pos_texts[pos_texts=="Salvini"])
pos_texts=="salvini"
sum(pos_texts=="salvini")
count(pos_texts=="salvini")
pos_texts[is.na(pos_texts=="salvini")]
is.na(neg_texts)
neg_texts <- neg_texts[!is.na(neg_texts)]
pos_texts <- pos_texts[!is.na(pos_texts)]
neg_texts <- neg_texts[!is.na(neg_texts)]
count(pos_texts=="salvini")
sum(pos_texts=="salvini")
sum(pos_texts=="Salvini")
data
data$lemma
lemmatized_texts <- c()
for (i in unique(data$doc_id)) {
tokens <- data[data$doc_id==i,]$lemma
tokens <- tolower(tokens[!tokens %in% sw])
#text <- paste(tokens, collapse=' ')
lemmatized_texts <- c(lemmatized_texts, list(tokens))
}
lemmatized_texts[1]
neg_texts <- unlist(lemmatized_texts[data_annotato$Style=='negative'])
pos_texts <- unlist(lemmatized_texts[data_annotato$Style=='positive'])
pos_texts <- pos_texts[!is.na(pos_texts)]
neg_texts <- neg_texts[!is.na(neg_texts)]
sum(pos_texts=="Salvini")
sum(pos_texts=="salvini")
sum(neg_texts=="salvini")
calc_sentiment("salvini")
length(neg_texts)
length(pos_texts)
calc_probs <- function(tokens){
counts <- table(tokens) + 1
counts/sum(counts)
}
calc_probs <- function(tokens){
counts <- table(tokens) + 1
counts/sum(counts)
}
pos_probs <- calc_probs(unlist(pos_texts))
pos_probs <- calc_probs(unlist(pos_texts))
neg_probs <- calc_probs(unlist(neg_texts))
calc_probs_rare <- function(tokens){
tokens <- unlist(tokens)
counts <- table(tokens) + 1
return(log(1/sum(counts)))
}
pos_probs_rare <- calc_probs_rare(pos_texts)
neg_probs_rare <- calc_probs_rare(neg_texts)
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test]))*pos_probs_rare+sum(pos_probs[test], na.rm=TRUE)
neg_pred <- sum(
is.na(neg_probs[test]))*neg_probs_rare+sum(neg_probs[test], na.rm=TRUE)
ifelse(pos_pred>neg_pred, "positivo", "negativo")
}
frase <- "essere"
t <- unlist(tokenize_words(frase))
neg_probs[t]
calc_sentiment("salvini")
calc_sentiment("salvini sarebbe secondo me un ottimo premier")
calc_sentiment("salvini sarebbe secondo me un pessimo premier")
neg_probs["pessimo"]
neg_probs[neg_probs=="pessimo"]
neg_probs
neg_probs["premier"]
neg_probs["ottimo"]
calc_sentiment("salvini è stato un pessimo premier")
calc_sentiment("salvini è stato un bravissimo premier")
neg_probs["bravissimo"]
calc_sentiment("siamo pronti a ripartire")
calc_sentiment("siamo pronti a ripartire per un futuro migliore")
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test]))*pos_probs_rare+sum(pos_probs[test], na.rm=TRUE)
neg_pred <- sum(
is.na(neg_probs[test]))*neg_probs_rare+sum(neg_probs[test], na.rm=TRUE)
cat("prob. pos.", pos_pred)
cat("prob. neg.", neg_pred)
}
calc_sentiment("siamo pronti a ripartire per un futuro migliore")
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test]))*pos_probs_rare+sum(pos_probs[test], na.rm=TRUE)
neg_pred <- sum(
is.na(neg_probs[test]))*neg_probs_rare+sum(neg_probs[test], na.rm=TRUE)
cat("prob. pos.", pos_pred)
print('\n')
cat("prob. neg.", neg_pred)
}
calc_sentiment("siamo pronti a ripartire per un futuro migliore")
print(cat("prob. neg.", neg_pred))
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test]))*pos_probs_rare+sum(pos_probs[test], na.rm=TRUE)
neg_pred <- sum(
is.na(neg_probs[test]))*neg_probs_rare+sum(neg_probs[test], na.rm=TRUE)
print(cat("prob. pos.", pos_pred))
print(cat("prob. neg.", neg_pred))
}
calc_sentiment("siamo pronti a ripartire per un futuro migliore")
calc_sentiment <- function(frase){
test <- unlist(tokenize_words(frase))
pos_pred <- sum(
is.na(pos_probs[test]))*pos_probs_rare+sum(pos_probs[test], na.rm=TRUE)
neg_pred <- sum(
is.na(neg_probs[test]))*neg_probs_rare+sum(neg_probs[test], na.rm=TRUE)
cat("prob. pos.", pos_pred, '\n')
cat("prob. neg.", neg_pred, '\n')
}
calc_sentiment("siamo pronti a ripartire per un futuro migliore")
calc_sentiment("siamo pronti a ripartire per un futuro peggiore")
calc_sentiment("salvini")
remove_punct <- function(text){
text <- tolower(text)
text <- gsub(". ", " ", text, fixed=TRUE)
text <- gsub(": ", " ", text, fixed=TRUE)
text <- gsub("? ", " ", text, fixed=TRUE)
text <- gsub("! ", " ", text, fixed=TRUE)
text <- gsub("; ", " ", text, fixed=TRUE)
text <- gsub(", ", " ", text, fixed=TRUE)
text <- gsub("\ `", " ", text, fixed=TRUE)
text <- gsub("\n ", " ", text, fixed=TRUE)
return(text)
}
my_data$clean_text <- remove_punct(my_data$Text)
my_data$clean_text
remove_punct <- function(text){
text <- tolower(text)
text <- gsub(".", " ", text, fixed=TRUE)
text <- gsub(":", " ", text, fixed=TRUE)
text <- gsub("?", " ", text, fixed=TRUE)
text <- gsub("!", " ", text, fixed=TRUE)
text <- gsub("; ", " ", text, fixed=TRUE)
text <- gsub(", ", " ", text, fixed=TRUE)
text <- gsub("\ `", " ", text, fixed=TRUE)
text <- gsub("\n ", " ", text, fixed=TRUE)
return(text)
}
my_data$clean_text <- remove_punct(my_data$Text)
# scriviamo una funzione per estrarre info sintattiche
annotate_splits <- function(x, file) {
ud_model <- udpipe_load_model(file)
x <- as.data.table(udpipe_annotate(ud_model,
x = x$Text,
doc_id = x$id))
return(x)
}
my_data$clean_text
remove_punct <- function(text){
text <- tolower(text)
text <- gsub(".", " ", text, fixed=TRUE)
text <- gsub(":", " ", text, fixed=TRUE)
text <- gsub("?", " ", text, fixed=TRUE)
text <- gsub("!", " ", text, fixed=TRUE)
text <- gsub("; ", " ", text, fixed=TRUE)
text <- gsub(", ", " ", text, fixed=TRUE)
text <- gsub("\ `", " ", text, fixed=TRUE)
text <- gsub("\n", " ", text, fixed=TRUE)
text <- gsub("\r", " ", text, fixed=TRUE)
return(text)
}
my_data$clean_text <- remove_punct(my_data$Text)
my_data$clean_text
# scriviamo una funzione per estrarre info sintattiche
annotate_splits <- function(x, file) {
ud_model <- udpipe_load_model(file)
x <- as.data.table(udpipe_annotate(ud_model,
x = x$clean_text,
doc_id = x$id))
return(x)
}
# load parallel library future.apply
library(future.apply)
# numero di core da utilizzare
ncores <- 3L
plan(multiprocess, workers = ncores)
#prima inseriamo una colonna per identificare gli id dei testi nel dataframe
my_data$id <- seq(1:nrow(my_data))
# dividere il corpus
corpus_splitted <- split(my_data, seq(1, nrow(my_data), by = 1000))
annotation <- future_lapply(corpus_splitted, annotate_splits, file = '../materiali/italian-isdt-ud-2.5-191206.udpipe')
annotation <- rbindlist(annotation)
write.csv(annotation, 'annotazioni-sintattiche.csv')
